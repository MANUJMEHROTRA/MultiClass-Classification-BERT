{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning DistilBERT for Multiclass Text Classification\n",
    "\n",
    "## Model - 'distilbert-base-uncased'\n",
    "\n",
    "\n",
    "## Dataset Link - https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First What is BERT?\n",
    "\n",
    "BERT stands for Bidirectional Encoder Representations from Transformers. The name itself gives us several clues to what BERT is all about.\n",
    "\n",
    "BERT architecture consists of several Transformer encoders stacked together. Each Transformer encoder encapsulates two sub-layers: a self-attention layer and a feed-forward layer.\n",
    "\n",
    "### There are two different BERT models:\n",
    "\n",
    "- BERT base, which is a BERT model consists of 12 layers of Transformer encoder, 12 attention heads, 768 hidden size, and 110M parameters.\n",
    "\n",
    "- BERT large, which is a BERT model consists of 24 layers of Transformer encoder,16 attention heads, 1024 hidden size, and 340 parameters.\n",
    "\n",
    "\n",
    "\n",
    "BERT Input and Output\n",
    "BERT model expects a sequence of tokens (words) as an input. In each sequence of tokens, there are two special tokens that BERT would expect as an input:\n",
    "\n",
    "- [CLS]: This is the first token of every sequence, which stands for classification token.\n",
    "- [SEP]: This is the token that makes BERT know which token belongs to which sequence. This special token is mainly important for a next sentence prediction task or question-answering task. If we only have one sequence, then this token will be appended to the end of the sequence.\n",
    "\n",
    "\n",
    "It is also important to note that the maximum size of tokens that can be fed into BERT model is 512. If the tokens in a sequence are less than 512, we can use padding to fill the unused token slots with [PAD] token. If the tokens in a sequence are longer than 512, then we need to do a truncation.\n",
    "\n",
    "And that’s all that BERT expects as input.\n",
    "\n",
    "BERT model then will output an embedding vector of size 768 in each of the tokens. We can use these vectors as an input for different kinds of NLP applications, whether it is text classification, next sentence prediction, Named-Entity-Recognition (NER), or question-answering.\n",
    "\n",
    "\n",
    "------------\n",
    "\n",
    "**For a text classification task**, we focus our attention on the embedding vector output from the special [CLS] token. This means that we’re going to use the embedding vector of size 768 from [CLS] token as an input for our classifier, which then will output a vector of size the number of classes in our classification task.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "![Imgur](https://imgur.com/NpeB9vb.png)\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\40104197\\anaconda3\\lib\\site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.0\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\40104197\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\40104197\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   -------------------------------------- 294.9/294.9 kB 587.0 kB/s eta 0:00:00\n",
      "Downloading pandas-2.2.1-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 11.6/11.6 MB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas, seaborn\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.11.2\n",
      "    Uninstalling seaborn-0.11.2:\n",
      "      Successfully uninstalled seaborn-0.11.2\n",
      "Successfully installed pandas-2.2.1 seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\40104197\\anaconda3\\Lib\\site-packages\\~-ndas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\40104197\\anaconda3\\Lib\\site-packages\\~~ndas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.1.0 requires joblib~=1.0.1, but you have joblib 1.2.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\40104197\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:44.342740Z",
     "iopub.status.busy": "2022-09-23T08:06:44.342112Z",
     "iopub.status.idle": "2022-09-23T08:06:44.347039Z",
     "shell.execute_reply": "2022-09-23T08:06:44.346050Z",
     "shell.execute_reply.started": "2022-09-23T08:06:44.342700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\40104197\\anaconda3\\lib\\site-packages (4.22.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip  install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:44.349259Z",
     "iopub.status.busy": "2022-09-23T08:06:44.348566Z",
     "iopub.status.idle": "2022-09-23T08:06:44.452061Z",
     "shell.execute_reply": "2022-09-23T08:06:44.451086Z",
     "shell.execute_reply.started": "2022-09-23T08:06:44.349178Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:44.454848Z",
     "iopub.status.busy": "2022-09-23T08:06:44.454516Z",
     "iopub.status.idle": "2022-09-23T08:06:44.461681Z",
     "shell.execute_reply": "2022-09-23T08:06:44.460702Z",
     "shell.execute_reply.started": "2022-09-23T08:06:44.454814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.22.1\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:44.464333Z",
     "iopub.status.busy": "2022-09-23T08:06:44.463179Z",
     "iopub.status.idle": "2022-09-23T08:06:47.251269Z",
     "shell.execute_reply": "2022-09-23T08:06:47.250110Z",
     "shell.execute_reply.started": "2022-09-23T08:06:44.464293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\40104197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopw = stopwords.words('english')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:47.253574Z",
     "iopub.status.busy": "2022-09-23T08:06:47.252641Z",
     "iopub.status.idle": "2022-09-23T08:06:47.259139Z",
     "shell.execute_reply": "2022-09-23T08:06:47.257888Z",
     "shell.execute_reply.started": "2022-09-23T08:06:47.253540Z"
    }
   },
   "outputs": [],
   "source": [
    "root_path = 'bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:47.261633Z",
     "iopub.status.busy": "2022-09-23T08:06:47.260640Z",
     "iopub.status.idle": "2022-09-23T08:06:47.330921Z",
     "shell.execute_reply": "2022-09-23T08:06:47.329991Z",
     "shell.execute_reply.started": "2022-09-23T08:06:47.261592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(root_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T08:06:47.332851Z",
     "iopub.status.busy": "2022-09-23T08:06:47.332489Z",
     "iopub.status.idle": "2022-09-23T08:06:47.339025Z",
     "shell.execute_reply": "2022-09-23T08:06:47.338074Z",
     "shell.execute_reply.started": "2022-09-23T08:06:47.332815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of the count of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "      <th>encoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>737</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>246</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>341</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "      <td>633</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  count  \\\n",
       "0           tech  tv future in the hands of viewers with home th...    737   \n",
       "1       business  worldcom boss  left books alone  former worldc...    300   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...    246   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...    341   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...    260   \n",
       "5       politics  howard hits back at mongrel jibe michael howar...    633   \n",
       "6       politics  blair prepares to name poll date tony blair is...    269   \n",
       "7          sport  henman hopes ended in dubai third seed tim hen...    191   \n",
       "8          sport  wilkinson fit to face edinburgh england captai...    157   \n",
       "9  entertainment  last star wars  not for children  the sixth an...    237   \n",
       "\n",
       "   encoded_text  \n",
       "0             4  \n",
       "1             0  \n",
       "2             3  \n",
       "3             3  \n",
       "4             1  \n",
       "5             2  \n",
       "6             2  \n",
       "7             3  \n",
       "8             3  \n",
       "9             1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['count'].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# data = df['count']\n",
    "# fig = px.histogram(df['count'], nbins=len(data), histnorm='probability density', title='Distribution Plot')\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize= (8, 8))\n",
    "\n",
    "# sns.displot(df['count'].fillna(0))\n",
    "\n",
    "# # plt.xlim(0, 1000)\n",
    "\n",
    "# # plt.xlabel('The num of words ', fontsize = 16)\n",
    "# # plt.title(\"The Number of Words Distribution\", fontsize = 18)\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar plot for each of the new category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sport', 'business', 'politics', 'tech', 'entertainment'], dtype='object', name='category')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count = df['category'].value_counts()\n",
    "\n",
    "categories = category_count.index\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sport', 'business', 'politics', 'tech', 'entertainment'], dtype='object', name='category')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAFZCAYAAACi11jlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABWyElEQVR4nO3dd3gc1dXH8e9RtST33m2CK2CqaaG30Duhhl4SAuSFUEMSQigJCSH0GnoJPWCKqcamG7ApNjbuvRdZVu/n/WNG9nq9klb2SivJv8/z7KOdO3dmzszOrs7evXPH3B0REREREUmMlGQHICIiIiLSmijBFhERERFJICXYIiIiIiIJpARbRERERCSBlGCLiIiIiCSQEmwRERERkQRSgi3SgpnZ/mbmZnZOsmNpLsxsnJnNS3Ycm8rMfmtm08ysLHxtByY7pqZmZueE+75/smORlv+eEkkGJdgizUiYVMT7GJjseCWxzOwA4H5gGvAb4ExgZR31x4Xnwhwzy4gx/8Zw/shGC7oVMbNOZnaDmX1jZnlmVm5mi8zsVTM7wcxsE9d7ub4Ei2xZ0pIdgIhs4Myo6X2Ai4BHgE+j5q0EBjZBTNJ0Dgn/nufuuQ1YbivgYuDuxIe0ZTCz3YBRQHfgDeA5IB/oAxwBvApcAjywCau/HJgHPLn5kSbFL4BN+nIhsqVSgi3SjLj7s5HTZpZGkGB/GT0vnN9UoUktzKyduxckaHU9ARqYXJcAc4E/mdnjCYxli2FmPYE3gTbAfu7+WVSVm83sUKBTkweXJGaWCmS6e7G7lyc7HpGWRl1ERFoJMzvXzKaEfXfnm9k1tdQbaWavmdmqsO50M/tjmMzHsx03syfNbE8z+9jMisxstZk9amZto+rG7LtpZgPD9dwYUbauP3nYD3m6mZWa2WQzOyqsM8LM3jWz/HCb95hZei1x/szMRpnZ2rD+a2b2sxj1zMwuNrOJZlZsZoVmNjbsrhEzZjM7JaxfAtwbxzE7zsw+D49VYfj82Oh1A+dGHGM3s3H1rRuoBv4AdAVivuYx4sk0s+vD86U07A7xppntFFWnxMyeilr24TC2u6PKXwyPc1o43dnM7jSz2eE2VofH7Op4Ygylhcd7fniuTjKzU6O2+4OZLTCzjf6fmdkvw1jPqmc7VxO0XF8bI7kGwN3fc/cXItZ9ipm9EW67LHw/vW5m20fF4MAAYD+rpYtXQ96TZnZiuM+l4bb/YmYHW4xrMcysq5ndb2YLLejusjCc7hJVr6bP+8Fm9mczmw2UAieH82t7Hw82s2fMbGm4/nlmdruZ5UTV62dmj0e8jivM7AszOzvmqyHSCqgFW6R1+A3QA3gMyAN+BfzDzBa5+39rKpnZkcD/gFnAHUAusCdwE7Aj8Ms4t7cj8BbwBPBfYH/gfIJk76LN3JdLCFoKHyX4J/874DUz+yXwH+B54HWCn60vA1YAt0StIwcYB3xFkHwOBn4L7GFmO7n7soi6zwCnAa+E+5MJnAF8YGYnuPsbUes+LozpQeAhgm4EtTKz37K+X/VNYfE5wOtm9mt3f4Sgu8+ZBMduH9Z3FVpe17pruPsbZvYZcIWZ3R+1f9HxpAPvAj8n2Pf7gA7AhcDnZravu09w9zIz+wI4IGoVBxG8zgdGrNMIzoFP3b0yLH4Z2JfgGE0CsoDhYb3b49kv4B8Er2VNt4xzgefNrI27PxmW/YfgS84hwHtRy58PrA1jqcuJQDnwVD31Il0KrCbovrUM2Jrg9fvczHZ295lhvTOBO4FVwK0Ry6+Ehr0nzewUgvN/NvBXoBI4Gzg6Ojgz6wB8AQwCHge+BXYi6Ep0oJntFuPXjn8B6QTHNB+YXtvOm9kuwEcEnzcPA4uBHQjeG3uZ2X7uXhF+SfiAoKvNA8AMgvNte4JzvSHHXKTlcHc99NCjmT4IEjEHzqll/v7h/CVAh4jybIJ/4F9GlLUhSAQ+AdKi1nNFuJ7944jJCRKs3aPK3wYqgLYRZeOAeTHWMTBcz40x9mVx1L5sH7HNE6LWMxFYGlU2Lqx/V1T58WH5QzHKLoqqmwZMIOh6YVExVwDD43z9OgGFBMlT+4jy9gRJUgHQMaL8yeBjOe7zYxxQGD7/eYz9uzEsGxnjtT40al3tgQXAuIiyP4Z1B4fT/cPpZ8K/PcLyEeH0leF0h3D6gc087+dHnQsdwrJcICss6wgUAy9FraMfUFVfDEC7cFuTGhhjToyy4UBZ9DYJ+l+Pi1E/7vdkeE4uJvjS1SmiXltgDlGfEwTJvAO/jVrvJWH5zTGO93Qgu5bzbF5U2Q8EXxrb1fI+Oyfq/XvNppwLeujRUh/qIiLSOjzh7mtrJty9GBhP0HJb4xCCVu4ngI7hz8ddzawrMDqs84s4t/elu38VVfYRQRIwcBPij/Rk1L5MImhNW+Lu/4uq+xnQ06K6poRui5xw99cIEojjIop/RZDkvh51PDoS9MkdyIbHEOBtd/8pzn05hKAF9h53X9fSHT6/hyA5OjjOddXJ3b8gaNk/38yG1FH1VwSJ0cSofc4gaGnc28yywrofhX8PjPhbxfrEvab8gKj6JQSJ5u62eaPdPBh1LqwlaBHvRPCFDHfPA14Cjo3q+nAuQTfIx+rZRvvwb52/RERz9yJY18WofXgMVxKcY7vHuZqGvCd3AXoTvD/WRMRRSHBMoh0fxvNIVPnDYfnxMZZ5MPzsqJOZjSBInP8LZEbF/RlQFBF3zet3gJl1r2/dIq2FEmyR1mFOjLLVQGTCMTz8+zjBP9jIx7RwXo/N3B5R29wUsda9hqA1OVZ5rG3meexuEj8BPSL6iA4naMFczsbH5MawTvQxmVFX8FG2Cv9OiTGvpmyjfuGb4Q8Eoz38vY46w4FhbLy/K4HzgFSC/twA3xB8AYlMsCe4+2xgclR5LvA9gAcXxV0ObAfMtaCv971mdlAD9yfWF5mp4d/I4/YIwReEM2Fdl5Vzge/dfWI926hJrNs1JDAz28nM3iI4PmtZfwxHEP/FkA15T9acS7G6bcQq2wqY7uu77AAQTs8g9nkX77ldE/dfY8S9guBLZY9we/MJWtN/ASy1oB/+P81s1zi3JdIiqQ+2SOtQFUedmiFHriZMhGJYkoDtRQ5t4rXUqeuzp7Z1x7vNhjCCpOD0Our8GDVdbwtfsrj7NDN7ArjAzGprRTWC5Pj3daxqZbi+SjP7lKD10QgS6afDOh8RtBqnAPsBH7n7utfb3R8ys1HAkeH8k4BLzexFd9/gQsXN5e5fmNmPBH2u7yLoJz6QoJ90fcsWmNl8YJiZZbl7SX3LmFl/gm4d+cDNBAluEWHXJIJfJuKRyPdkIsR7btfEfQdBf/5YIlvZ/2RmjxOcC/sAFwBXm9k/3f3aTQ1WpDlTgi2y5ai56KrI3T9som3mEvy0HS2RrbaxdDSznjFasYcDK2p+3ic4JkOA8eFP7YlW0xq/LTAmat42UXUS5S8EXxj+QdB3NtpMoBtBQlwdx/o+IhgH+iSCC9Vq9mMMQSv1CQRdaj6KXtDdlxJcrPqoBcO+PQOcZmZ3uPs3cWx7OMHY1JFqO27/Ae62YDzr8wkukH0ujm1AcJHhFQQt4NFdKmI5niCJPsbdx0bOCLuplEXVr+2LZkPek/PCv0NjzItVNgcYamZpka3Y4UWHQ9i8864m7qp4P0vcfQ7Bxaj3mlkbggtSrwnPhRWbEYtIs6QuIiJbjvcIfr69zsw6R880sywza9DP5HGYAbQLk56a7aQQJDON7brICTM7niAReT2i+GmCz8GYXSrMLN4uM7X5gKBl87LIYxs+v4zgAsgPNnMbG3D3JQQ3nNmPIDGO9jTBeNsxW7Bj7HNN4vxXgsTx83D6E4JfFf4aVQ8zyzaz7Ki4qghGEwHY6PyrxcXhaBg16+1AMGJOHvBxVN1nCJLqqwkS4FfD/tnx+CdBq/0/zWzPWBXM7Be2fojAml9TLKrOhYRjmUcpJPY+N+Q9OQFYCpxjZp0i6rQlOCbRXif4InVBVPmFYflrMZaJ13cEv+z8xmIPfZlWsz9m1sGihtJ091LWd//ZYsYWly2LWrBFthDuXmTBeMCvA9PDn2xnEbQ+DiNoiTye2K2em+oR4EqCYfbuJhgK7SQa/7NnFXCCmfUm2J+aYfqWs75vNe7+Stil4lIz25lg6MFVQF+CodIGsRmt7e6eZ8F45PcDX5nZk+Gsc8J1/zryIr4E+gfBkHGx+rneTXBx3e1mdiBBYpxPMELIQQRJauTQfN8T/BIxnGAkjFIILtQ0swkEF/QtjbrwcwjwsZm9RpCIrQmXv5igL330XUlrs4rguD0RTp8bxnlB9MV47r7GzF4huIgTgpbzuLj7MgvGWh8FfGZmr7O+C0hv4DBg7zB+gHcIulM8Y2b3hfu3F8EXmtlsfH6PJ7j49GaCxLIaeLMh78mwu85VBK3yX5vZYwTD9J1DcP3DVmzYUv5PgiH+7g/P7e8Ihuk7n6BLyz/jPT4xjpeb2ZkE586kMO4pBKMXDQrj/gPBqDgHAI+Y2avhdgsJftW6APjK3WsdClCkRUv2MCZ66KFH7Q/iH6Zvo/nUMuQbwYVnzxIM+VVOkHR+AfwZ6BxHTE4wkkFtse4fVX4EQZJWRtCf9B8ELcm1DdMXa1/mEXuYsxvDZQZGlI0L6/+MIGHKJ7gQbRQwqJZ9OpMg6csnSDDnEXQbOCWizsDomBvwOh4fHuOi8PEFcFy8r1kd6x1HOExfjHk1w7xtMExfOC+NYLzibyJimkmQvP0ixrpeDdfz56jymqHgnosq70Iw9vP3BK3NJQSJ411Arwac9wcTtJAvCM+fycDpdSy3T7jcTMLhFRv4OnUm6GIzgeDCxXJgEcEY6cdE1d2XYMSMgnAf3yZ4b41j4yHtuofHMJcguY4+Z+N+TxIkzZPC47EgjLdmaLyTo+p2Ixh7ehHB8JKLCL7sdY3nvRv9nopRPoBgBJN5YdyrCYbO/DvQL6yzVVjnJ4L3V1H4/CYihmDUQ4/W9qgZ31VERKRFC7sifQVc7+51jaTSqpjZlQQ3idnT3ccnOx4RQQm2iIi0Dmb2NHAq0N/ruJtlS2VmGQQXFlZFlLUlaNFuD/T2YIhEEUky9cEWEZEWKxzT/GiCkVp+BTzSGpPr0M+Ad8zsBYK+7L0IbpW+FXCxkmuR5kMt2CIi0mKFd4qcS3Dx3DsEF0A26K6MLUU4BOB9BBdUdie4yHEycKe7v5TM2ERkQ0qwRUREREQSSONgi4iIiIgkUKvrg921a1cfOHBgssMQERERkVZu4sSJq9y9W3R5q0uwBw4cyIQJE5IdhoiIiIi0cmY2P1a5uoiIiIiIiCSQEmwRERERkQRSgi0iIiIikkBKsEVEREREEkgJtoiIiIhIAinBFhERERFJICXYIiIiIiIJpARbRERERCSBlGCLiIiIiCSQEmwRERERkQRSgi0iIiIikkBpyQ6gOdjl6qeTHYI0wMTbz2qS7Sy4aUSTbEcSo/8Nk5MdgoiICKAWbBERERGRhGryBNvM5pnZZDP73swmhGWdzewDM5sZ/u0UlpuZ3WNms8xskpnt3NTxioiIiIg0RLJasA9w9x3dfWQ4fR0wxt0HA2PCaYDDgcHh4yLgwSaPVERERESkAZpLF5FjgafC508Bx0WUP+2B8UBHM+uVhPhEREREROKSjATbgffNbKKZXRSW9XD3peHzZUCP8HkfYGHEsovCMhERERGRZikZo4js7e6Lzaw78IGZTYuc6e5uZt6QFYaJ+kUA/fv3T1ykSdAxJ5NendqSkdY8flyoqKxmWV4RuYWlyQ5FREREpEVo8gTb3ReHf1eY2WvAbsByM+vl7kvDLiArwuqLgX4Ri/cNy6LX+QjwCMDIkSMblJw3F2kpxiE7DKRLuywWri6grKIy2SEB0D4rlR0GdqewtJz3vp9LeWV1skNq8Swjm8x+O5LWeSApGVlgluyQGpc71WVFVKyeS/nC7/HKsmRHJCIi0qiaNME2sxwgxd0Lwue/AG4C3gDOBm4L/44KF3kDuNTMXgB2B9ZGdCVpVX4+rA9llVU89+lUvBl+Rdh7WB/226YfH0yan+xQWjTLyCZnx+OoXD2f4qnvUV1aQNBrqvUySyElqwMZvbcjZ4djKPx+FFSVJzssERGRRtPULdg9gNcsaLFLA/7r7u+a2TfAS2Z2PjAfODmsPxo4ApgFFAPnNnG8TSI1xfhZj4689MW0ZplcA3w1cym/2ncbMtJS1Iq9GTJ6b0vlmkWUzv482aE0GQeqClZQMv0jsrc5lIwegylfMiXZYYmIiDSaJk2w3X0OsEOM8tXAQTHKHbikCUJLqnZtMigtr6S4rHl0C4mloqqavKIyOua0YcXa4mSH02KldR5A6azPkh1G0pSvmElGz+FKsEVEpFVrHlfSbeFSUoyq6mbadB2hsrqa1JRW3l+4kaWkZ1FdVpTsMJLGy4qw9DbJDkNERKRRKcGWdY4euTWvX3tcssPYAmz8Zar3794le8RRCd9S+70voNup9yZ8vZvOW/9FnSIissVLxjB90gylqWW6Vcr/7NFkhyAiIrLFUYLdzJ261zBO33c4HbPbUFRWwVsTZvO/r2bw1vUncvPLX3DW/tvRpW0bJsxZzi0vf8maomC86g7Zmfz+mJHsMaQ3AOOnL+GON74hvyQYveHNP5zAG9/MYuTWPdmmXxceHzOZCw/ZgfTUFD695TQALn/8IybOWZ6cHRcRERFpoZRgN2P9u7bjsiN25sx73mbO8rW0bZPOwO4d1s0/cpetufDBdyktr+Kvp+7FLafvzSX/+RCAW07fm8qqak66PRjx8JbT9ubm0/bm/x7/aN3yx+0+mN8/MZbpS3LJTEtlVUEJ5x80guP+8XqT7qcE0jr1pfs5T5HRcxgVq+ayZvTNlC+ZQudjboHqSnLfunFd3d6/e5e8sfdRPPktUjv0pvNRN5DZZwQ4VOYtYtX/rqVy9Tw67Hcxmf12ZsWzF65brvDbV2iz1e5k9NmeyrzF5L71V8oX/bBu3Tk7nUi73c8grX1PKtcsIm/MnZTO+RKA9J7D6HTYH8joPhj3aipXzWXF85fgpflkb3sYHfa9mNT2PfCKUkpmf0buqD816TEUERFpDpRgN2NV1Q4GW/foyNI1RRSWVvDjglX06pQDwCMf/MDqgqDF+q63JjLquuPp2j4LgJ8P7cNx/3iNgrDF+t9vTuB/1xxH13ZZrCooAeD1r2YyfUkuAGWVVU29exKl7S6/ZOULl1GxfAbt9zybbqc9wJJ7j6h3uY4H/h9Va5ey6IXLoLqK9G6DqC7Jr7V+zo7Hs+rF31Gxai4dD7mKLsfeytL7g/7fOTudSPu9zmPVy1dQsXwmbQbtTdeT72LZwydRuWYhnQ//IyWzP2fFU+eCpZDRaxuoqsDS2tDluL+x4rnfUDbvayw9i4xewxN2bERERFoSXeTYjC3OLeRP//2M43YfzLt/PonHfnsYewzptW7+0jWFGz3v0SGbnh2CBHxJ7vr5i1YXBPM7Zq8rWxKxvCRf0XevUbF0KlRXkv/5Y3hlGVlD9qt3Oa+qILVtV9I69QWvpmLFDKqLc2utXzjxZSpWzgavpui7V0nvMgDLbAtAu93PIP+Th6hYPgNwSmd9Stm8r8ne7vB120pr35PU9j2hupLyxZPwiuALm1dXkt51K1LatMcrSihb8O3mHxQREZEWSC3YzdzYHxcw9scFpKWmcNIeQ7jjnAP41V1vA9CrU1sWrS5c9xxgecQY1cH8ILHu0zmcn7d+fnXUXW2ip6VpVeYt3nB67VJS2/eod7m8D++gwz6/ptup95GSnkXxTx+QN+audYlvtKrCVeueV5cHdVIycqgqKyStYx86Hf5HOh123foFUtKozA/64q9+48902OfX9Dj3aaiqoGjy26z9+EG8spSV//0t7fY4i44H/I7KNYvIH/8UxT+ObuhhEBERafGUYDdjA7q1p3fntnw3ZzmlFVUUlpaDg4eJ8AUHb8/sZXmUVVTxuyN3ZvyMJazKDxKmL6cv4YqjR/KXFz7DzLji6JF89tOidd1DYlldUEKntm3IyUynqKyiSfZR1kvr2GfD6Q69qMpfTlr7nqRkd1w/w1JJyem8brK6eA1r3rsN3ruN1I596XbKPbTf6zzWjru/wTFUrV1K3rgHKPnp/djz8xaT++YNAKR3H0z3Mx6mMm8RRd+/Ttn8CZTNnwCWQtaQ/el68p2UL55E5ZpFDY5DRESkJVOC3Yylp6Zw4cHbs3WPjgAsXF3A1U+PW9df+p1v5/Dobw+jS9s2fDt3OTe8sP4OgX96/lOuPGZX/nfNcQCMn7GEf78xoc7tTZi1jK9mLuWNP5xAaorx+yfH8q1GEWkyOTseR/G0MVSsmEm7Pc7E0ttQMvMTLDWdjgdfQWrHPlQVrKDj/pdiKevfutnbHErZkh+pyluMlxVAVQVevWl96vPHP0OH/S6mMnc+FcunY2mZZPTahqriPCpXzyVn+2MonfMlVYUrqS7ND7ZTXU1KThcy++1E6dzxeFkh1WXBLydeXZ2QYyMiItKSKMFuxmYty+O8+9/dqLzmIscvpi/h9a9nxVw2r6iMPz9f+y25j/77/zYqq6x2rnn6402MVjZX4bev0Omw68joOYzKVfNY+fwleFkhRZPfJrPfTvS68CWqK0rI/+xRqgpWrFsuvddwOh5yJSlZHfDyYkpmjKPgiyc3KYai716Fqgq6HHMzaZ364FWVlC/7ibwP7gAgc6vd6HjQ5VhmDtWlBRRPfpuiSW+S0rYL7XY9lS5H/xVSUqnKX8bq1/9I1doliTg0IiIiLYoSbJFmYMk9hwGQ/8lDG88Mh+iLHKavcMIL656vHXMXa8fcFXO9az9+MOZ2alStXcKCm0ZsUFY06Q2KJr0Rc321DbtXXbiKFc9cEHOeiIjIlkajiIiIiIiIJJBasFugpWuK2OXqp5MdhoiIiIjEoBZsEREREZEEUoItIiIiIpJA6iLSwtx4ys85fKefUR5xa/N73v6Wl7+cDsDgXp247IidGdqnM13bZXH+/e/y/bwVG6zjzyftyYgB3RjQrT1vTZjNza982aT7ICIiItKaKcFugepKiiuqqhk7eQEPvvc9z/7fkTHrzFy6hg8nzeeEPQY3ZpjShDofcws5I47Aq9bfICjvw39TOOFFANJ7DKHjQVeQ0XMYqW27svyJsyhb+N0G60jr1I/OR95ARt/tqS4toGD80xSMV19/ERGRhlKC3crMW7GWeSvW1lnnhc+nAXDoTgObICJpKkU/vLHBUH6RvKqC4p8+ZO3Ye+l54YsbV7AUup16H6Vzx7PyhctI67oV3c94kKr85RRPfa9xAxcREWlllGC3QAeO6M8BI/qTV1TGx1MW8sgHP1BSXpnssKQZq1w1l8pVc2udnzlgF1I79iJvzF14ZSkVy36icOIrtB15shJsERGRBlKC3cK88Nk07nn7W9YUlbJV9w785eS9+NNJe/LH/36a7NAkybKHH0LW8IOpLl5DyfSxrP34QbyiJK5lM3oMpXL1/A3qly+dSttdT22scEVERFotjSLSwkxbnEtuYSnuMGf5Wv795jccvP0A0lP1Um7JCr75L0seOIbFt+/DqpcuJ3PASDoffWPcy1tGDtVlhRuUVZcVkJKZk+BIRUREWj9lZS1ctTsAZkkORJKqYulUqotWA07Fytmsef+fZA8/BFLT41rey4tIyWy7QVlKZjuqy4oaIVoREZHWTQl2C/OLHQbStk2QNPXr2o4rjhrJx1MXUl5Zva5ORloKGWnBS5ueGjxPicjA0yLKUlKMjLQU0tQC3rqEX7wgvm9e5cunk9ZlAJaeta4so9dwKpZPb4TgREREWjf1wW5hTtxzCNedsDsZaSnkFpYy9seFPPL+D+vm9+qUw1vXn7hu+qHf/AKAG1/8nDcnzAbg/gsPZuTWPdfVOWbXQUyYvYxfP/R+E+2FJFr2todRMutzvKyAtM796XTIVZTMGAdV5esrpWZEPE8Ppqsrwaspmz+RqryldDzwd+SNuYu0LgNpu/MvWfPebU2+LyIiIi2dEuwWpr4keOmaIna5uu6xi5VItz5tdzmZTkf8CUtNp7ool+LpH7F23APr5qd26E2f/1s/GkiPsx4DYPWoP1H0wyjwala+cCmdj7qBPld/SnVpAflfPkHxlHebfF9ERERaOiXYIq3AiqfPq3N+1dolLLhpRJ11KtcsZMUzFyYyLBERkS2SOt6KiIiIiCSQEuxmoKraW8RFhulpKVRWVddfUergYM3/tW48FnEBpoiISOu0Jf+nbzbyS8pIS02hXVZG/ZWTpE16Kh2yMllTVJbsUFq06vJiUtq0T3YYSZOS1R4v19B/IiLSuinBbgbcYcaSXPbdpu+64fWak7QUY59t+jF7eZ5asDdT5ep5ZPQcluwwksTI6DmMitXzkh2IiIhIo9JFjs3E17OWsvewPpyxzzasyC+mrKIKT/JP6WZGeloKPTvkMH9VPp9PW5zUeFqDssU/0naHY8ka/gsqlk+jurSg9XeZsBRSsjqQ0XtbwKhYMSvZEYmIiDQqJdjNhDt8+tNivp65jO4ds8lITU12SABUVFUxdu1CSisqkx1K61BVTuEPo8joNZzMfjthGdnEezOYlsvxskIqVs2lfNm0YOxtERGRVkwJdjNTVlnFwlUFyQ5DGlNVOeWLfqB80Q/11xUREZEWZ5M7/JrZMDM7zsx6JzIgEREREZGWLK4E28weNrOHIqZPASYD/wOmmdnPGyk+EREREZEWJd4W7MOATyKmbwaeB3oD74XTIiIiIiJbvHj7YHcHFgKY2WBgEHCCuy8zs0eAFxspPhGRZmWve/dKdggSp88v+zzZIYjIFireFuxcoEf4/GBgmbv/GE4b0DyGvBARERERSbJ4W7DfAW4ysx7ANcBLEfO2A+YlOC4RERERkRYp3hbsK4HxwG8I+mLfEDHveODdBMclIiIiItIixdWC7e5rgfNqmbdPQzdqZqnABGCxux9lZlsBLwBdgInAme5ebmaZwNPALsBq4BR3n9fQ7YmIiIiINJV4h+m72cwOMbO2Cdru/wE/RUz/A7jT3QcBa4Dzw/LzgTVh+Z1hPRERERGRZivePtjHAdcD1Wb2A/BpzcPdVzZkg2bWFzgSuBX4vZkZcCBweljlKeBG4EHg2PA5wCvAfWZm7u4N2aaISEuUlpLGkG5D6N+pP9np2QQfly1PVXUV+aX5zFk9h3lr5iU7HBGRRhdvF5ERZtYJ2CficQmQamYzCRLtC+Pc5l0EF0q2C6e7AHnuXhlOLwL6hM/7EA4P6O6VZrY2rL8qzm2JiLRIaSlpHDr0UMoqy5i6bCr5ZfnQQpsWUlJS6JLdhR377EiPdj34asFXyQ5JRKRRxduCjbuvAd4A3jCzDILh+q4B9gUGA/Um2GZ2FLDC3Sea2f6bEnAt670IuAigf//+iVqtiEjSDO42mPKqcj6c+WGyQ0mI3OJc5q+Zz/Ejjmf6yunkleQlO6SE+njf/ZIdgsRpv08+TnYIsgWIK8E2s/bAXqxvvR4JFAKfA1cTdBeJx17AMWZ2BNAGaA/cDXQ0s7SwFbsvsDisvxjoBywyszSgA8HFjhtw90eARwBGjhzZQtt4RETWG9BpAFOXT012GAlVXlXOvNx5DOg0oNUl2CIikRpyo5lXgIHAs8BId+/m7se5+x3u/nU8K3H3P7h7X3cfCJwKfOTuZwBjgZPCamcDo8Lnb4TThPM/Uv9rEdkSZKVnUVBakOwwEq6grIDs9OxkhyEi0qjiTbC/IWjtPhj4BXCIme1kibvi5lqCCx5nEfSxfiwsfwzoEpb/HrguQdsTEWnWDCPR7Qnn7XYedx1317rpfx3zL07f+fTaF4izTkNUe3Vw/18RkVYs3osc9zSzLGAPgj7XRwG3AJVm9gXwsbs3aAg9dx8HjAufzwF2i1GnFPhlQ9YrIiLxueqNqzaY/vyyz7n4lYuZtHRSrXVERKR+DbnIsYSgK8fYsE/2AcAVwGHAoWiMahERERGRuC9y7MmGQ/RtF86aAtxP/Bc5iojIZnjl7Fd4e+rb7Np/VwZ3HcyCNQu4fdztTFsxjVRL5ayRZ3H48MNpl9mOGStncNcndzE3d27Mdd17/L1MWDiBpyY8xZOnPQnAncfeSbVXM2bmGG776LYN6gD0bNeTS/a6hO17b09mWiZzV8/l2revJb80n4v2uIgjhx9JdkY2a0vX8sJ3L/DKpFea6tBIBEtLI3vAALL69iW1TZsm375XV1Oxdi1Fc+dSvrJBt8sQaRXibcFeApQD3wLvAX8CPnf3vEaKS0REanHcdsdxzVvXMHv1bE7d6VT+dcy/OPmpkzlx+xM5bNhhXPXmVSxdu5QzR57JXcfdxWnPnEZxRXGd6zzn+XP4/LLPuWLUFRt0EYmUmZbJPcffw/j54zn92dMprShlWPdhVFZVslu/3Th8+OFc+PKFrChcQcesjnTL6dYYuy/1sIwMeh56KFWlpRTNnUtVcTE08fgAlppKRpcudD/gAPKnTCF/ypQm3X5zcd+VbyY7BInTpXccndD1xZtgHwh8FXYTERGRJHpr6ltMXzkdgGcnPsvxI45nr6324ojhR/Dst8+yYM0CAJ74+gmO3vZofj7w5wkZT3uvgXuRmZbJ3Z/cTZVXATBleZA4VVRXkJGawVadtyKvJG/dQ5pehxEjKM/LY/Wnyf1xuWTRIgpnzaL3McdQNH8+VYWFSY1HpCnFNYqIu49z9xIL9DOzn5tZTmMHJyIiG1tasHSD6eUFy+nWths92vVgaf76eY6zNH8p3dt1T8h2e7bvyZK1S9Yl15G+W/wdD3/5MGfvejZvXfAWdx57J8O6D0vIdqVhsgcMoGBq82gxrioqonjBfLJ1EzjZwsQ7TB9m9luCG7/MJ+hzPTQs/5+ZXd4o0YmIyEZ6teu1wXSPdj1YWbiS5QXLN5hnGL3a92JFwYq41lvt1XXOX5a/jN7te5Nisf91vDHlDX776m85+rGjmblyJrcecWtc25XESmubQ0Xe2mSHsU7F2rWktW2b7DBEmlRcCbaZXQ38G/gPQXeRyFFMxwGnJDwyERGJ6chtjmRItyGkpqRy+s6n0yatDV/M+4J3pr3D6TufTr+O/UhLSePsXc8m1VL5Yt4Xca03tyiXvh371jr/i3lfUFFdwe/2+R05GTmkWirb9tiW7PRshvcYzg69dyA9JZ2KqgqKK4qprq47YZfGYZaS8DHUo/U47DB2++9zcdX1qmoSd9sMkZYh3j7YlwA3uPs/zSw1at50YEhiwxIRkdqMmjKKy/e9PBhFJG8BV795NUXlRTz37XOkp6Zz57F3kpORw6xVs7hi1BX1XuBY4+HxD3PB7hdw2d6X8dGsj7h97O0bzC+tLOV3r/2OS/e+lBfPfJG01DTmrJ7DtW9dS1Z6FpfudSn9OvajyquYs3oON7x3Q2PsvmyiHe6+izUTJ7Lg6WeSHYpIqxdvgt0TmFjLvGqg6ccAEhHZQi1eu5gnvn5io/Kq6ioe++oxHvvqsRhLweNfP77B9GWvXbbB9OifRjP6p9F11lmSv4TrR1+/0bq/XfQt5714Xlzxi2zJUtNSSEuPbquUplJd7VSUV0IjD6wTb4I9C9gPGBNj3r7A1IRFJCIiIgk16PL/o8P229N+223pf/rplK1axTe/OpOeRx1F35NOJLN7d0qWLGXuww+x5psJ65bruu8+9P/Vr8jq04fq8nKWvj2aeY8+um5+nxNPpN9pp5KSlcXKsWOZ+e87QV2DYmrfJZu+g7qSlZNBZUVVY+d3UovU1KC70qolBSyaubLRRrCMN8G+C3jAzMqBmrsGdDez84HfAxc2QmwiIiKSALPuupucrbbaoItIz6OOov/ppzHlzzdQNGcOnXffjW1vvpkJ519A6eLFdN59d4Zefz0//fUmcr/+mtTMTHK23nrdOjN79iS9Uye+Ou10Mrt3Z+eHH2LtDz+w4oPNHxKytWnXKYutR/Ri7tTlrF1Z2NTDkkuUzKx0Bgzrzs+268XsyUvrX2ATxDtM36PAH4FrCe7eCDAauBu40d3/2yjRiYjIBk566iTen/5+ssOQVqDvSScy/6mnKZo9G9zJHf8Ved99T/eDDgSg94knsHTUKHK//BKqqqgqLiZ/8uR1y1eXlTHv8cfxigpKFy8mb+K3tBs6NFm706z1HNCJhTNXkrdCyXVzUFZSwcwfltC+azYZbeJta26YuNfq7reb2UPAnkBXIBf40t2bz1hAIiIiEpc2vXox6IrLGfS79f3sLTWVsvDW5m169mTVJ7XfrKZizZoNuoNUlZaQmp3deAG3YO07ZzN36vJkhyERvNrJX11M+87ZrFqSn/D1Nyhtd/cCQE0nIiIiLUz00H2ly5cz7/EnWDVuXMz6pcuWkd239mEbJX6paSlUlm98g6aWbJeDBtFzQCfefvybZIeyySrLq0hNi/uWMA1Sa4JtZvs2ZEXu/snmhyMiIolmGA+e9CAjeo3guMePY2XRSgZ1HcRvfv4bhnQdQpecLlz8ysVMWjpp3TI79N6Bfx39rw3Wk5GWwbzceZz9/NlNvQuSAOWrc8nq02fd9KKXXmbguedQsmgRRbNmkZKRQduhQ6lYu5aSBQtY8r/XGH7jX8j77jtyJ0xY1wc7spuIJM+wXfsy8uAhPPv3jzZ7XQeduiPV1dWMfWlS/ZVDE8fM2uztNrZEHqOGqqsFexzBICZ1jQ7vEX8bpxOLiIhsllN2OoXSytINyiqqKvh49sc8Ov5RHjtl42H9fljyA4c8fMi6acN45exXeG/6e40erzSOxS+/zNDrrmWvt9+ibNUqJpx9Dl5ZwdDrriOrV0+qq6oonDGDOQ88CEDu+PHM+Mc/GXjhhQy/8S9UlZay7K23lGC3ImaNPlrdFquupHhEPcv2Ba4BDgDWJCwiERFJmH4d+3HCiBP44+g/8uRpT64rn79mPvPXzI97PXsO3JPOOZ0ZPXV0/ZWlWSqYNo0J55y7Qdnyd99j+bu1f2laOXYsK8eO3ah8+bvvsvzddzcom/732xIT6BYkLT2V3Q8bytYjepGRlcbyBXl88r/JrF1dzPEX78mKRWtp3zmbfkO6UVJYxmdvTGHulOX0HNCJ/U/cntTUFC762+EAvP3Y1yyevZrOPdux9zHb0K1PByorqpnx7SK+enc61dVOu05ZnP2ngxnz4vfstN/WdOiazYQPZzJk5+CXjcE7Bn//88d36NyzPfsevx2de7bDDJbPz+Pj1yaTvzq4cdVuvxhCr606M+rh8QCc9ceDmDJ+Pn0Hd6VH/04U5BYz9pVJLJsXpIgHnbojZkZ1dTVbj+hFRXkln785lTXLCznglzvQqXtbVizK44PnvqUov6ze4wNs8jFqkte2thnuPiVWuZkNBK4DzgbyCEYWeaARYhMRkc1gGH846A/c99l9FJQVbNa6jtvuOMbNGkdeaV5ighMRDjh5ezLbpPPyPZ9SVlLByIMHc9QFu/P87eMAGLZrP95+/GveeXoCO+zzMw4+bSee+OsHLJu/hnGvTtqo+0NW2wxO+O3P+XL0NN567GuycjI58rxdqayo4psPZq6rN2TnPrz+0JeUFpfj1U77Ljkxuog4X783naXz1pCWnsKBJ+/AL07fiVfu/bzW/Rm+W39GP/41a1YUstfR23LwqTvy7G3rv6AN2qEX7zw1gY9e+oFt9xjAAb/cgcWzVjH6yW8oK67gqPN3Y7dDhzL25Un1Hp/qat+kY9RU4u7ZbWZDzewpYAZwOHA1MNDdb3f3osYKUERENs3JO55MbnEun8zZvEtkerTtwR4D9mDUj6MSFJmItMnJYOjOfRn36mRKCsuprnK+fn8G2e0y6TGgEwAzv18ctAA7TBk/n8ysdDp0zal1ncNG9mXVknymjJ9PdZVTlF/KxDGzGDqy3wb1vnl/BsUFZVRXea3DBq5eWsDi2auprqqmvLSSr9+fQc+Bneu8C+WUL+eTuzwYinDqV/Pp2K3tBsPgLZq5ivk/rQCH6RMWkZGZxrQJiyhaW0plRRWzJi2he7+OcR+fTTlGTaXeftNmtj3wJ+BEYA5wMfC0u1c0cmwiIrKJ+nTow6k7ncr5L56/2es6etujWZC3gO+XfL/5gYkIAO07ZwFw2lX7bVCekppC245tACgOu0oA60YhqWvc5nads+m1VScuvOWwDcotZcPL6fJzS+qPr0s2ex21DT0GdCQjM21dIp7VNoOCNbGXL8pff61HRRhvemYa5aWVwfyC9fMrK4L5xQVlG5SlZwb7F8/xgYYfo6ZS1ygiuxMk1kcQ3Ar9TOAFd9c9UEVEmrkdeu1Ax6yOPHN6cNe+FAt+sHz69Kd5ZPwjvDb5tbjWk2qpHLXNUTz37XONFqvIlqgmSX3m7x9RWlS+0fzt9hhQ5/KxsrGCNSUsnLGKtx77uu5lo5qto6cB9j9pe4rXlvLCvz6mtLiCzj3bcfrV+9e53kSq7/jEI5kZa10p/pcEF5eOA/4HdAR+YxZzUBF39wcTHZyIiGyaMbPG8M3C9ePTdm/bnUdOfoQrRl2x7uLGjNSMdfPTUtPISM2gsrqS6oj/SntttRftMtvxzk/vNF3wIluAksJypn+7iP1PHMGnr0+hKL+UjDZp9B3UlYUzVta7fHFBGdltM0jPTKOiLGghnj5hETvttzXDd+vHjG8XU1VVTftO2XTslsOC6bWvszi/jJ4DOgXjxoW5dkZmGmvLqygrqaBNTga7H9q0d+ms7/hUxDGueKxj1FTqa0M3glFCDqinngNKsEVEmomyyjJWVq7/h5qaEvSbXF20mpKKEnq268mr57y6bv69x98LwK0f3MroaetHCjl2u2MZM3MMheWFTRS5NBkzdrz/Pjpstx1fnngS5eEdHHsceigDzjmbjC5dKJozh5n/vpPCGTMASMnIYNgf/0jO4EFk9e7NvMcfZ8HTzyRzL1q0sS9NYpeDBnH8b/cku10bykorWDpnNQviSLAXz1rFwhmrOPuPB2EpxtuPf8OSOat57cEv+PmRw9nj8GGkpadSsKaYH7+se8SgqV8toO/grlxw06GYGY/++V0+e2MKB5y0PRfdejgFeSV8N242W2/fK1G7HpfNOT5Q+zFqChbrZ4GWbOTIkT5hwoQGLbPL1U83UjTSGCbeflaTbGfBTfWNVCnNSf8bmmZs3r3u3atJtnPCiBP4aOZHrW7UjmHdh9EpuxNfzvuy0bf1+WW1j3aQaB/vu1/9lZrIgLPPZv4zz2xwG/NY+p5yMp13351OI0euS7DbjxjB9v+6nSl/+hN53/9A35NOou8pJ/P16WdQVVyMZWTQ+9hjKZo1i60uupDVX35Zb4Ldbvhw0tu3J/errxK5m5tsv08+brJt3Xflm0AwpN3X789osu1KfAYM605pcTnLF+Rx6R1Hb9I6zGyiu4+MLm+c+0OKiIhIs5XVty+9jzuO2Q9s+ONzr6OPYtUnn7Lmmwl4RQULn3+e6ooKuu67DwBeXs7il18m77vvqC7ftH6xIlsCJdgiIs1VXffRbaFquY5HmpIZQ6+7ltkPPEhl4YZdf9puvTUFM6ZvUFY4cyZttx7UlBGKtHhKsEVEmqHSylJyMpI/lmui5WTkUFZRVn9FaTR9TjqJ8txcVn/66UbzUrOzqSzc8NYWlYWFpOZkN1V4Iq2CEmwRkWZoUd4itu6ydbLDSKhUS2Vgp4EsyFuQ7FC2WG369KHfKScz8667Y86vKi4mre2GX+zS2ralqqi4KcITaTWSPxK3iIhsZPrK6Ry1zVHsOXBPpq+YTkFpAdW0zNsQpFoqXXO6sn3v7VldvJpVRauSHdIWq8OIEaR37MiuTz4RFKQE7Wwjn3iceY8+RuHs2bQdPGSDZdoOGsSqTzbvbqAiW5q6bjRTBezp7l+b2ePAze4+t+lCExHZcpVVljH6p9GM6DmCAwcdSHZGy/2J3t3JK81j7uq5TF0+NdnhbNFWjh3LmokT101nduvGzg89yKQrr6J4wQIKZ89m+9v/yfJ3d2btpEn0OelEUjIyWPXJ+u4klp4OZpCSgqWmYhkZUFWFV9U/LrE0X3scPowhO/WhTU46lRXVLJmzms/emEphXglmsMcRwxmycx8y26RTsKaYr9+fwexJS9ctn5aRyp5HDGPQDr1Jz0yjMK+E95/7llWL85O4V8lTVwt2OVBzF4JzgIcAJdgiIk2kpKKErxd+zdcL674rm0i8qsvK1o13DWCpwfjo5bm5VJeUkD95MjPvvIsh11y9bhzsyddcS1Xx+i4iuz37DG16BeMhd9xhBwaeey7L3nmH6X+/rWl3RhJq2sRFfDt2FuWllaSlp7LH4UM59MydefXezxmx11YM26Uvrz34BXkri9hqu54cduYuPL/sY/JWBBfKHnHurlRVVPHy3Z9SmFdK+87ZVJQ37c1dmpO6EuypwI1m9no4fZKZbTTOX0h3chQREWlhypYt22gM7+Xvvcfy996rdZmvTjm1scOSJKhJlAEwcIdO3doC0KFrNotnryJvZXAB7Nwfl1FaXE6Xnu3IW1FI/6Hd6DWwM0/e9AFlJRUA5Odu2f3260qwLwMeBu4kuFPjVXXU1Z0cRURERFqwITv1Yb8TR5CZlU5VVTWfvzEFgKnjF3DIGTvTqUdb8lYU8rMRvUhJsXV3RewzqAv5ucXsfthQBu3Qm4qySmZ+v4Sv35tOdXXruqFhvGpNsN39C2AEgJlVA3u4u36nFBERkfhp7PMWY8Z3i5nx3WKy22UyfLf+rF5aAMDa3GKWzF3N6Vftj7tTVVnNB89/R0lhcLOhNjkZdOnZjgXTVvDULR/StmMWR1+wGxXlVUwcMzOZu5Q08Q7TdwBBlxERERFpxqrKykjNykp2GOuktmmjuz62MMUFZUz9aj5Hnr8bmVnp7H/CCLr16cDTfxvDA9e+zahHxrP/idvTb0g3ACpKq6iuqubL0dOoqqxm7aoiJn8+j59t1yPJe5I8cSXY7v6xuxea2e5mdqWZ3Rr+3b2xAxQREZH4lS5dQvaAAckOI2BG9oD+lCxenOxIpIFSUoyMzDRyOrShW98OTJ+wiII1JeCwbN4als7NZcDw7gCsWrI25jp8y+wdAsSZYJtZjpmNBr4A/g6cF/79wszeNrOWO36UiIhIK5L/4xQ6bL89bYcMCYbUS5L0Dh3out9+VBWXULZiRdLikDgYjNhrIFltg8Hjcjq0Yd8TRpC/upg1KwpZOm8NQ3bpS077NgD06N+RPlt3YeWiILGePXkpJUXl7H7YEFJSjXads9ju5wOZM3lprZts7eK90cw/gT2BU4FX3b3azFKAEwkuhPwHwUWRIiIikkTlq1ez4sMP6bDDDnTeY3e8qppgLIKmYykpVJeVUzR3TsxbskvzM2B4d3Y9ZAjpGamUlVawePZqXn/4S7za+fzNqex11HB+efneZGSmU1xYxncfz2b6xEUAVJRV8cYj49nvhBFcePNhlBSVM+2bhXw3bnaS9yp54k2wTwSudfeXawrcvRp42cw6ATehBFtERKRZKF+1ipVjxoAZlp5OU19m6NXVeOWWOwZyi+Pw1qO1j2NRUVbJuFcnw6uTa62zemkB/7v/i8aIrkWKN8HuACysZd5CoH1iwhEREZGEccfLy5u4/VpE4h1F5AfgYrMNx9oJpy8O54uIiIiIbPHibcG+HngHmGZmrwHLge7A8cBA4PBGiU5ERESkhXN3zLbsUTWao+A1aZwXJd5h+j4Cdga+A34J3AqcDHwL7OzuY+NZj5m1MbOvzewHM5tiZn8Ny7cys6/MbJaZvWhmGWF5Zjg9K5w/sOG7KCIiIpI8ZSUVZLfLTHYYEiWrXSZlxRWNsu54u4jg7lPc/VR339rds8O/p7t7Q25AUwYc6O47ADsCh5nZHgSjkNzp7oOANcD5Yf3zgTVh+Z1hPREREZEWI3d5IT0Hdk52GBKhXacs2uRkkJ9b3Cjrj7eLSEJ40A5fGE6mhw8HDgROD8ufAm4EHgSODZ8DvALcZ2bmjdWeLyIiIpJgS+asZujOfdlm9/7kLiugvKxS/UWSJCU1hXadsujYvS2zf1jaaC9DkybYAGaWCkwEBgH3A7OBPHevGc9nEdAnfN6HcPQSd680s7VAF2BVkwYtIiIisomqq5xpExbSoWsOHbrmkNOhTbJD2mK5O8X5ZSyePZ/y0sYbSrLJE2x3rwJ2NLOOwGvAsM1dp5ldBFwE0L9//81dnYiIiEhCuUPeyiLyVhYlOxRpAnH3wU40d88DxhLcIbKjmdUk+32BxeHzxUA/gHB+B2B1jHU94u4j3X1kt27dGjt0EREREZFa1ZtghyN5nGFmgzd3Y2bWLWy5xsyygEOAnwgS7ZPCamcDo8Lnb4TThPM/Uv9rEREREWnO6k2w3b0MeBTonYDt9QLGmtkk4BvgA3d/C7gW+L2ZzSLoY/1YWP8xoEtY/nvgugTEICIiIiLSaOLtgz0ZGAJ8vDkbc/dJwE4xyucAu8UoLyUYd1tEREREpEWIN8G+AnjSzJYC70aM+CEiIiIiIhHiTbBfB7IJ+ka7ma0hGL96HXfvntjQRERERERanngT7PuJSqhFRERERGRjcSXY7n5jI8chIiIiItIqNOhGM2bWCdiOYGzqd9x9jZm1AcrdvboxAhQRERERaUniutGMmaWZ2T8JbmP+MfAMsFU4+1XgL40TnoiIiIhIyxLvnRxvBS4ELgV+BljEvFHA0QmOS0RERESkRYq3i8hZwHXu/oSZpUbNm02QdIuIiIiIbPHibcHuSJBIx5IBRCfdIiIiIiJbpHgT7B+BY2uZdzjwbWLCERERERFp2eLtInIL8KqZZQEvE4yJvaOZHQ/8GjimkeITEREREWlR4mrBdvdRwOnAwcA7BBc5PgqcA5zp7u81VoAiIiIiIi1J3ONgu/tLwEtmNhToAuQC091dd3gUEREREQk16EYzAO4+vTECERERERFpDeK9yBEzG2Fm/zWzWWZWFP79r5lt35gBioiIiIi0JHG1YJvZccBLBEP1vQKsALoTjCwywcxOdvfXGylGEREREZEWI94uIv8guGPjyZF9rs3sDwSjivwDeD3h0YmIiIiItDDxdhHpBzwafUFjOP2fcL6IiIiIyBYv3gR7ArBtLfO2QzeaEREREREB6ugiYmbZEZO/B14ws3SCriA1fbCPBy4ATm3EGEVEREREWoy6+mAXEtyxsYYBfwf+FlUG8BWQmtjQRERERERanroS7PPYMMEWEREREZF61Jpgu/uTTRiHiIiIiEirEPeNZkREREREpH7x3mgmA7ic4KLGPkCb6Dru3j2hkYmIiIiItEDx3mjmQeAMgpvNfASUN1pEIiIiIiItWLwJ9gnA5e7+UGMGIyIiIiLS0sXbBzsXWNCYgYiIiIiItAbxJtg3AVeaWU5jBiMiIiIi0tLF1UXE3Z8ys22ABWY2EcjbuIqfkujgRERERERamnhHEbkSuBpYBuQA6Y0ZlIiIiIhISxXvRY7XAfcAV7i77u4oIiIiIlKLePtgG/CWkmsRERERkbrFm2A/CZzYiHGIiIiIiLQK8XYRWQT83sw+JLjRTF7UfHf3BxMZmIiIiIhISxRvgv3v8G9f4MAY853gbo8iIiIiIlu0eIfpi7criYiIiIjIFk2Js4iIiIhIAsU7DvYR9dVx99GbH46IiIiISMsWbx/stwj6WVtUeeSwfakJiUhEREREpAWLN8HeKkZZJ+BQ4FzgnEQFJCIiIiLSksV7keP8GMXzge/NrAq4HjgmkYGJiIiIiLREibjI8TtiD923ETPrZ2ZjzWyqmU0xs/8Lyzub2QdmNjP82yksNzO7x8xmmdkkM9s5AfGKiIiIiDSazUqwzSyDoHvI0jgXqQSudPdtgD2AS8xsG+A6YIy7DwbGhNMAhwODw8dFaKxtEREREWnm4h1F5Bs2vKARIAMYCLQj6IddL3dfSpiMu3uBmf0E9AGOBfYPqz0FjAOuDcufdncHxptZRzPrFa5HRERERKTZifcixylsnGCXAi8Dr7v7lIZu2MwGAjsBXwE9IpLmZUCP8HkfYGHEYovCsg0SbDO7iKCFm/79+zc0FBERERGRhIn3IsdzErlRM2sLvApc7u75ZutH/3N3N7PoZL6++B4BHgEYOXJkg5YVEREREUmkJr+To5mlEyTXz7n7/8Li5WbWK5zfC1gRli8G+kUs3jcsExERERFplmptwTazxxuwHnf38+urZEFT9WPAT+7+74hZbwBnA7eFf0dFlF9qZi8AuwNr1f9aRERERJqzurqIjIhj+TbAtgT9s+tNsIG9gDOByWb2fVh2PUFi/ZKZnU8wvvbJ4bzRwBHALKCYOC+mFBERERFJlloTbHfftbZ5ZpYD/Bb4PVBOMPJHvdz9Mza+3XqNg2LUd+CSeNYtIiIiItIcxDuKCABm1gH4XfjIAh4Fbnd39YsWERERESH+cbC7AFcStFobwQ1f7nD3lY0Ym4iIiIhIi1Nngh2O6HE1wRjTZcCdwN3untf4oYmIiIiItDx1jSLyIMFt0POAvwIPuHtR04QlIiIiItIy1dWC/evw72rgl8AvI28IE83dd0tgXCIiIiIiLVJdCfbTbHx7dBERERERqUNdw/Sd04RxiIiIiIi0Ck1+q3QRERERkdZMCbaIiIiISAIpwRYRERERSSAl2CIiIiIiCaQEW0REREQkgeK6VXoNM+sEbAf0A95x9zVm1gYod/fqxghQRERERKQliasF28xSzeyfwCLgY+AZYKtw9qvAXxonPBERERGRliXeLiJ/Ay4ELgV+BkTe0nEUcHSC4xIRERERaZHi7SJyFnCduz9hZqlR82YTJN0iIiIiIlu8eFuwOxIk0rFkANFJt4iIiIjIFineBPtH4Nha5h0OfJuYcEREREREWrZ4u4jcArxqZlnAy4ADO5rZ8cCvgWMaKT4RERERkRYlrhZsdx8FnA4cDLxDcJHjo8A5wJnu/l5jBSgiIiIi0pLEPQ62u78EvGRmQ4CuQC4w3d29sYITEREREWlpGnSjGQB3nwHMaIRYRERERERavLgTbDPrDRwF9AXaRM12d782kYGJiIiIiLREcSXY4cWMzxMMx7cCKI+q4oASbBERERHZ4sXbgv034H3gHHfPbcR4RERERERatHgT7H7AZUquRURERETqFu+NZr4AhjZmICIiIiIirUGtLdhmlh0x+XvgOTMrBD4A8qLru3txwqMTEREREWlh6uoiUkhw8WINA56IKouUmqigRERERERaqroS7HObLAoRERERkVairgR7LvCtuxc2VTAiIiIiIi1dXRc5jgW2aapARERERERag7oSbGuyKEREREREWol4h+kTEREREZE41HejmSPMbFg8K3L3pxMQj4iIiIhIi1Zfgn1DnOtxQAm2iIiIiGzx6kuwDwAmNEUgIiIiIiKtQX0Jdom7FzVJJCIiIiIirYAuchQRERERSSAl2CIiIiIiCVRrFxF3V/ItIiIiItJATZpEm9njZrbCzH6MKOtsZh+Y2czwb6ew3MzsHjObZWaTzGznpoxVRERERGRTNHUr9ZPAYVFl1wFj3H0wMCacBjgcGBw+LgIebKIYRUREREQ2WZMm2O7+CZAbVXws8FT4/CnguIjypz0wHuhoZr2aJFARERERkU3UHPpZ93D3peHzZUCP8HkfYGFEvUVhmYiIiIhIs9UcEux13N0J7grZIGZ2kZlNMLMJK1eubITIRERERETi0xwS7OU1XT/CvyvC8sVAv4h6fcOyjbj7I+4+0t1HduvWrVGDFRERERGpS3NIsN8Azg6fnw2Miig/KxxNZA9gbURXEhERERGRZqm+W6UnlJk9D+wPdDWzRcBfgNuAl8zsfGA+cHJYfTRwBDALKAbObcpYRUREREQ2RZMm2O5+Wi2zDopR14FLGjciEREREZHEag5dREREREREWg0l2CIiIiIiCaQEW0REREQkgZRgi4iIiIgkkBJsEREREZEEUoItIiIiIpJASrBFRERERBJICbaIiIiISAIpwRYRERERSSAl2CIiIiIiCaQEW0REREQkgZRgi4iIiIgkkBJsEREREZEEUoItIiIiIpJASrBFRERERBJICbaIiIiISAIpwRYRERERSSAl2CIiIiIiCaQEW0REREQkgZRgi4iIiIgkkBJsEREREZEEUoItIiIiIpJASrBFRERERBJICbaIiIiISAIpwRYRERERSSAl2CIiIiIiCaQEW0REREQkgZRgi4iIiIgkkBJsEREREZEEUoItIiIiIpJASrBFRERERBJICbaIiIiISAIpwRYRERERSSAl2CIiIiIiCaQEW0REREQkgZRgi4iIiIgkkBJsEREREZEEUoItIiIiIpJASrBFRERERBJICbaIiIiISAIpwRYRERERSSAl2CIiIiIiCdTsE2wzO8zMppvZLDO7LtnxiIiIiIjUpVkn2GaWCtwPHA5sA5xmZtskNyoRERERkdo16wQb2A2Y5e5z3L0ceAE4NskxiYiIiIjUqrkn2H2AhRHTi8IyEREREZFmydw92THUysxOAg5z9wvC6TOB3d390qh6FwEXhZNDgelNGmjz1RVYlewgpNnReSGx6LyQWHReSCw6L9Yb4O7dogvTkhFJAywG+kVM9w3LNuDujwCPNFVQLYWZTXD3kcmOQ5oXnRcSi84LiUXnhcSi86J+zb2LyDfAYDPbyswygFOBN5Ick4iIiIhIrZp1C7a7V5rZpcB7QCrwuLtPSXJYIiIiIiK1atYJNoC7jwZGJzuOFkrdZiQWnRcSi84LiUXnhcSi86IezfoiRxERERGRlqa598EWEREREWlRlGCLNCNmNs/MDk7g+vYxMw1bKeuYmZvZoPD5Q2b25zrqXm9mjzZddNISmNn+ZrYo2XFIcunzoW5KsAUAM3vSzG5JdhySWO7+qbsPTXYc0jy5+2/c/WaInTS5+99q7kMgrUOiv8RL85CI19XMxplZ3O/35vr50ND9aCzN/iJHaXxmlprsGERERKTpmZkBluw4Whu1YLcAZnatmS02swIzm25mB5nZjWb2ipm9GJZ/a2Y7RCwzPPwWl2dmU8zsmIh5T5rZg2Y22syKgPOBM4BrzKzQzN5Mwm7Kerua2VQzW2NmT5hZGzM7x8w+i6wU9VP/EeEyBeG5clVYvkGrZNjKcZWZTTKzteH50yZi/lFm9n143nxhZttHzNvoPAzLdzOzCWaWb2bLzezfjX2AZN1r+YfocyWcd6GZzTKzXDN7w8x617KOJ83sFjPLAd4BeoefAYVm1jv8nHk2ov7e4XmRZ2YLzeycsDzm+SfNi5k9A/QH3gxf42vMbI+I1/QHM9s/on7n8LxaEp5jr0et70ozW2FmS83s3CbdmVYqfN+9amYrzWyumf0uLL/RzF4ys6fD99kUMxsZztvodQ3L63ptx5nZrWb2OVAMPAPsA9wXruO+sN7d4Xs938wmmtk+EetY9/lgZgPD/0lnm9kCM1tlZn+MqvuymT0bxj/ZzIaEn2Erwm38IqJ+BzN7LDy3FoefU6nhvHPM7DMz+1d4Xs41s8PDebfG2o+kcHc9mvGD4NbvC4He4fRAYGvgRqACOAlIB64C5obP04FZwPVABnAgUAAMDdfxJLAW2IvgS1absOyWZO/vlv4A5gE/EtzBtDPwOXALcA7wWVRdBwaFz5cC+4TPOwE7h8/3BxZFrf9roHe4/p+A34TzdgJWALsTjDt/dlg/s7bzMHz+JXBm+LwtsEeyj+OW8KjjXDmQ4BbGO4ev3b3AJ7WcN+ve99HnSlh2I/Bs+HxA+DlyWvgZ0wXYsa7zT4/m9wjPm4PD532A1cAR4f+CQ8LpbuH8t4EXw9c0Hdgv4lypBG4Ky48gSNI6JXv/WvIjfA0mAjcQ/O/+GTAHODR8L5aGxzoV+DswPtbrGudrOw5YAGxL0JshPSy7ICqmX4Xv9TTgSmAZ0CacF/n5MDD8bPkPkAXsAJQBwyPqlob7kgY8TZCz/DHc9oXA3IjtvgY8DOQA3Qn+b/06nHcOQf5zYXgsLgaWsH5kvI32IxkPtWA3f1UE/yS3MbN0d5/n7rPDeRPd/RV3rwD+TZAo7xE+2gK3uXu5u38EvEXwj7HGKHf/3N2r3b206XZH4nCfuy9091zgVjZ83WpTQXCOtHf3Ne7+bR1173H3JeH63wR2DMsvAh5296/cvcrdnyL4gNyDus/DCmCQmXV190J3H9/gPZZNFetcOYPgplzfunsZ8AdgTzMbuJnbOh340N2fd/cKd1/t7t+H8xpy/knz8StgtLuPDv8XfABMAI4ws17A4QRfwNeEr/nHEctWADeF5aOBQoIv4rLpdiVIgG8K/3fPIUhYTw3nfxa+VlUELc471LYi6nhtI+o86e5T3L0yzCM24u7Phu/1Sne/g/UNLrX5q7uXuPsPwA9RMX7q7u+5eyXwMtCNIE+pAF4ABppZRzPrEcZ5ubsXufsK4M6I4wAw393/Ex6Lp4BeQI864mpySrCbOXefBVxO8O1vhZm9EPFz78KIetXAIoKWyd7AwrCsxnyCb7RELyvNTuRrM5/g9azPiQQfSPPN7GMz27OOussinhcTfBmDoIXyyvDnxDwzyyNoHe1dz3l4PjAEmGZm35jZUXHEK4kR61zpHT4HwN0LCVqu+rB5+gGza5nXkPNPmo8BwC+j3vN7EyQr/YBcd19Ty7Krw0SpRuRniWyaAQTdtCJfj+tZnzhGf3a3MbParqWr67WtUW8eYEGXwp8s6FKYB3QAutaxSG3/XwCWRzwvAVaFCXLNNGH9AQSt2ksjYn+YoCV7o+24e3HEss2GEuwWwN3/6+57E5x0DvwjnNWvpo6ZpQB9CX4mWQL0C8tq9AcWR642ejOJjls2Wb+I5/0JXs8iILum0Mx6Ri7g7t+4+7EEH0CvAy9twnYXAre6e8eIR7a7Px9uI+Z56O4z3f20cNv/AF6xoE+vNL5Y58oSgtcIgPC16MKG7/9Y6vsMWEjQPW3jBRNz/knTiHydFwLPRL3nc9z9tnBeZzPrmJQot0wLCbpJRL4e7dz9iHqX3Pj9W9drW9syG0yH/a2vAU4m6P7TkaB7aWNfELmQ4NfTrhGxt3f3beNcvlnkM0qwmzkzG2pmB5pZJkH/pRKgpmV6FzM7IfwGeznBCTke+Irgm+M1ZpYeXthwNMFPMLVZTtDfS5LvEjPra2adCfqnvUjwU9u2ZrajBRey3VhT2cwyzOwMM+sQ/tSWz/pzpCH+A/zGzHa3QI6ZHWlm7eo6D83sV2bWLfzFJC9c16ZsXxou1rnyPHBueK5kAn8DvnL3efWsaznQxcw61DL/OeBgMzvZzNLMrEu4jUSdf9I0Ij/rnwWONrNDzSzVgguq9zezvu6+lODC1wfMrFP4v2TfpEW9ZfgaKLDggvKs8DXZzsx2jWPZ6P/htb62DVhHO4K+9iuBNDO7AWjfsF1quPDcex+4w8zam1mKmW1tZvvFuYpmkc8owW7+MoHbCC5aWkbQQvSHcN4o4BRgDXAmcELYH66cIKE+PFzuAeAsd59Wx3YeI+hDmWdRV4pLk/svwYfLHIKf5G9x9xkEFxR9CMwEPota5kxgnpnlA78h6IfbIO4+geCikfsIzqlZBBeTQN3n4WHAFDMrBO4GTnX3EqQpxDpXPgT+DLxKcPHh1mzYdzGm8PPheWBO+DnQO2r+AoJuIFcCucD3rO9fudnnnzSZvwN/Cn92PwU4lqAbwkqClsOrWZ8bnEnQ13oawQXQlzdxrFuUsLvEUQTXxcwl+Lx9lKBbRn3Wva5mdpW7L6Tu1zaWu4GTLBiZ4x7gPeBdYAZBt7NSmq576VkEF3pOJfh/9Aobdm+pS/R+JEXNFZfSwpjZjQQjAfwq2bGISNMzs3kEV8p/mOxYRERkQ2rBFhERERFJICXYIiIiIiIJpC4iIiIiIiIJpBZsEREREZEEUoItIiIiIpJASrBFRERERBJICbaISBMzsxPN7KNwzNoyM5thZv+OHnu6nnVcE95EqlkysyfNbEKy4xARSQZd5Cgi0oTM7A6CG3Y8QXCzqHxgG4IbtMxx9+PjXM8q4D53v7FxIt08ZrY1kOXuPyY7FhGRppaW7ABERLYUZnY08HvgfHd/PGLWx2b2CPCL5ESWOGaW5e4l7j472bGIiCSLuoiIiDSdK4Bvo5JrILhNsru/A2Bmt5nZZDMrNLNFZvacmfWsqRvexbEL8Bcz8/CxfzgvxcyuM7NZEd1Pzo7clgVuNrMVZpZvZo+b2anhegZG1OtqZk+Z2WozKzazcWY2Mmpd88zsDjP7s5ktImiRj9lFxMz6m9kLZpYbru89MxsaVecPYeylZrbczN6N3HcRkZZACbaISBMws3Tg58C7cVTvDvwNOJKgO8nPgI/MrOYz+3hgLfAYsGf4+Dacdy/wJ+CRcPnXgMfN7KiI9V8OXA88BJwElAD/jBHH68ChwFXAKQT/M8aa2aCoeqcD+wG/DettxMw6A58BQwm6w5wM5AAfmllWWOesMK5/h9u9GJgV1hMRaTHURUREpGl0ATKBBfVVdPfzap6bWSrwJbAI2Bv4xN2/M7NKYJG7j4+oO4ggKT3X3Z8Kiz80s17AX4C3wvVdAzzk7jeEdd43s62AfhHrOgzYC9jf3T8Oyz4C5gFXA7+OCvsody+tY7euIEiUd3T33HB9n4frOw+4H9gNeN/dH4hY7n91rFNEpFlSC7aISNOq98pyMzvczL4ws7VAJUFyDTCknkUPAqqB18wsreYBjAF2DJPrfkBP4I2oZaOndwNW1CTXAO5eBLxFkOhHGlNPcg1wMPABkB8RVwEwEajpdvI9cISZ/dXMdgvjFRFpcZRgi4g0jdVAGdC/rkpmtitBsrsIOJOg+8ce4ew29WyjK5BK0H2kIuLxJMEvlr0IkmuAlVHLRk/3AlbE2MZyoHOMsvp0Jeg+UhH1OID1LeePE3QRORn4ClhuZrco0RaRlkZdREREmoC7V4RdIg4l6CNdm+MJkt1TPBxH1cwGxLmZXIIW770IWrKjrWD95363qHnR00sJ+oJH6xFuJ1I8473mEnxxuDnGvAIAd68G7gTuNLN+wBnArQRfNh6KYxsiIs2CWrBFRJrOXcDI6FE9YN3oH4cBWUCFb3iTgjNirKucjVu0PyJowe7g7hNiPMqBhcAy4NioZY+Jmv4K6G5m+0bEmE1w4eRn9e1oDGOAbYEpMeKaHl3Z3Re6+20EFzluswnbExFJGrVgi4g0EXd/08z+DTxmZnsR3GimEBhGMLLGPOA/wOVmdhfwJsHII7+KsbppwJFm9m64junuPt3MHgJeMLN/AhMIkvBtgSHufoG7V5nZ7cDtZrYS+JwguR4Rrrc6jPU9M/sCeNHMriPo4nIVwReA2zdh9/8d7sdHZnYvsJigNXw/4DN3f97MHiZo6R5P0M3lAGAwcO0mbE9EJGnUgi0i0oTc/UqCvsiDgf8SXPh3JUEL78XuPpogoTyRoEvFfsBRMVZ1NVAEvA18A+wSll9C0A3jLGA0Qf/rI4FPIpa9E/g7wbB6rwKdCIYFhHAc69BxYXx3AS8DBhzo7rM2Yb9XEfQlnxZu/32CoQE7AJPCal8C+xLc5XI0QXeZC9399YZuT0QkmXSrdBERwcweBQ5x93j7e4uISC3URUREZAtjZtsRtKJ/QdAl5HDgXNQVQ0QkIdSCLSKyhQlvKvM4sCPBzV/mAw8Dd7j+KYiIbDYl2CIiIiIiCaSLHEVEREREEkgJtoiIiIhIAinBFhERERFJICXYIiIiIiIJpARbRERERCSBlGCLiIiIiCTQ/wMIhKHtKpzGWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize= (12, 5))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.barplot(x = category_count.index, y = category_count )\n",
    "\n",
    "for a, p in enumerate(ax.patches):\n",
    "    ax.annotate(f'{categories[a]}\\n' + format(p.get_height(), '.0f'), xy = (p.get_x() + p.get_width() / 2.0, p.get_height()), xytext = (0,-25), size = 13, color = 'white' , ha = 'center', va = 'center', textcoords = 'offset points', bbox = dict(boxstyle = 'round', facecolor='none',edgecolor='white', alpha = 0.5) )\n",
    "    \n",
    "plt.xlabel('Categories', size = 15)\n",
    "\n",
    "plt.ylabel('The Number of News', size= 15)\n",
    "\n",
    "plt.xticks(size = 12)\n",
    "\n",
    "plt.title(\"The number of News by Categories\" , size = 18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "      <th>encoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>737</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>246</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>341</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "      <td>633</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "      <td>269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "      <td>191</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  count  \\\n",
       "0           tech  tv future in the hands of viewers with home th...    737   \n",
       "1       business  worldcom boss  left books alone  former worldc...    300   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...    246   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...    341   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...    260   \n",
       "5       politics  howard hits back at mongrel jibe michael howar...    633   \n",
       "6       politics  blair prepares to name poll date tony blair is...    269   \n",
       "7          sport  henman hopes ended in dubai third seed tim hen...    191   \n",
       "8          sport  wilkinson fit to face edinburgh england captai...    157   \n",
       "9  entertainment  last star wars  not for children  the sixth an...    237   \n",
       "\n",
       "   encoded_text  \n",
       "0             4  \n",
       "1             0  \n",
       "2             3  \n",
       "3             3  \n",
       "4             1  \n",
       "5             2  \n",
       "6             2  \n",
       "7             3  \n",
       "8             3  \n",
       "9             1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoded_text'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts = df['text'].to_list()\n",
    "\n",
    "data_labels = df['encoded_text'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test SPlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size = 0.2, random_state = 0 )\n",
    "\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size = 0.01, random_state = 0 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5cf6e65eac4bbc8b0ec0dd0b3d36f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e623b4d768364327b65d91d7686f02f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88f056d2727461eb8e4201574a83a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for key,item in train_encodings.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "# Load DistilBERT model\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "# Load DistilBERT model\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Create a dummy input\n",
    "input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n",
    "\n",
    "# Call the model on the input\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Create a TensorFlow graph model\n",
    "graph_model = tf.keras.Model(inputs=input_ids, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\40104197\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Downloading pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(graph_model, to_file='distilbert_graph.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32860/2760332589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Extract weights and biases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mweights_and_biases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_weights_and_biases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Write weights and biases to TensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32860/2760332589.py\u001b[0m in \u001b[0;36mextract_weights_and_biases\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_weights'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mweights_and_biases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_weights'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mweights_and_biases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_biases'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "# Load DistilBERT model\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Create a summary writer\n",
    "writer = tf.summary.create_file_writer(\"logs\")\n",
    "\n",
    "# Define a function to extract weights and biases\n",
    "def extract_weights_and_biases(model):\n",
    "    weights_and_biases = {}\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'get_weights'):\n",
    "            weights, biases = layer.get_weights()\n",
    "            weights_and_biases[layer.name + '_weights'] = weights\n",
    "            weights_and_biases[layer.name + '_biases'] = biases\n",
    "    return weights_and_biases\n",
    "\n",
    "# Dummy input\n",
    "input_ids = tf.keras.Input(shape=(128,), dtype='int32')\n",
    "\n",
    "# Call the model on the input\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Create a TensorFlow graph model\n",
    "graph_model = tf.keras.Model(inputs=input_ids, outputs=outputs)\n",
    "\n",
    "# Extract weights and biases\n",
    "weights_and_biases = extract_weights_and_biases(graph_model)\n",
    "\n",
    "# Write weights and biases to TensorBoard\n",
    "with writer.as_default():\n",
    "    for name, value in weights_and_biases.items():\n",
    "        tf.summary.histogram(name, value, step=0)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['attention_mask'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_encodings['attention_mask'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in train_encodings['input_ids'][4]:\n",
    "    if i!=0:\n",
    "        count = count+1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 16509,\n",
       " 3415,\n",
       " 6332,\n",
       " 2000,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 16509,\n",
       " 2038,\n",
       " 2315,\n",
       " 1037,\n",
       " 6480,\n",
       " 7904,\n",
       " 2000,\n",
       " 2448,\n",
       " 2049,\n",
       " 3136,\n",
       " 2044,\n",
       " 5828,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2049,\n",
       " 3811,\n",
       " 3144,\n",
       " 5795,\n",
       " 3138,\n",
       " 3715,\n",
       " 2012,\n",
       " 14605,\n",
       " 1012,\n",
       " 2004,\n",
       " 2708,\n",
       " 4082,\n",
       " 2961,\n",
       " 2000,\n",
       " 6182,\n",
       " 19663,\n",
       " 11895,\n",
       " 3654,\n",
       " 2097,\n",
       " 2448,\n",
       " 16509,\n",
       " 2006,\n",
       " 1037,\n",
       " 3679,\n",
       " 3978,\n",
       " 2348,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2040,\n",
       " 3040,\n",
       " 23356,\n",
       " 2098,\n",
       " 2049,\n",
       " 7233,\n",
       " 2097,\n",
       " 3961,\n",
       " 2708,\n",
       " 3237,\n",
       " 1012,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2003,\n",
       " 2000,\n",
       " 2468,\n",
       " 3472,\n",
       " 1998,\n",
       " 2708,\n",
       " 3237,\n",
       " 1997,\n",
       " 14605,\n",
       " 2029,\n",
       " 8617,\n",
       " 4008,\n",
       " 1003,\n",
       " 1997,\n",
       " 1996,\n",
       " 2887,\n",
       " 2482,\n",
       " 8571,\n",
       " 1999,\n",
       " 2258,\n",
       " 1012,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 8590,\n",
       " 16509,\n",
       " 2046,\n",
       " 1037,\n",
       " 3435,\n",
       " 1011,\n",
       " 3652,\n",
       " 1998,\n",
       " 15282,\n",
       " 2449,\n",
       " 1012,\n",
       " 2720,\n",
       " 11895,\n",
       " 3654,\n",
       " 2097,\n",
       " 24207,\n",
       " 3710,\n",
       " 2004,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 1055,\n",
       " 4112,\n",
       " 1012,\n",
       " 2174,\n",
       " 2002,\n",
       " 2097,\n",
       " 2022,\n",
       " 16509,\n",
       " 1055,\n",
       " 2087,\n",
       " 3026,\n",
       " 2900,\n",
       " 1011,\n",
       " 2241,\n",
       " 3237,\n",
       " 1998,\n",
       " 2097,\n",
       " 2022,\n",
       " 1999,\n",
       " 3715,\n",
       " 1997,\n",
       " 1996,\n",
       " 3813,\n",
       " 1055,\n",
       " 3795,\n",
       " 4341,\n",
       " 1998,\n",
       " 5821,\n",
       " 1012,\n",
       " 2002,\n",
       " 2003,\n",
       " 2747,\n",
       " 1999,\n",
       " 3715,\n",
       " 1997,\n",
       " 16509,\n",
       " 1055,\n",
       " 3136,\n",
       " 2408,\n",
       " 4021,\n",
       " 1998,\n",
       " 17151,\n",
       " 21493,\n",
       " 15396,\n",
       " 1998,\n",
       " 2003,\n",
       " 5827,\n",
       " 2007,\n",
       " 6022,\n",
       " 9229,\n",
       " 2049,\n",
       " 4341,\n",
       " 1999,\n",
       " 2859,\n",
       " 1012,\n",
       " 2002,\n",
       " 2097,\n",
       " 22490,\n",
       " 1037,\n",
       " 2844,\n",
       " 8027,\n",
       " 2013,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2040,\n",
       " 2038,\n",
       " 22485,\n",
       " 1037,\n",
       " 6918,\n",
       " 2735,\n",
       " 24490,\n",
       " 1999,\n",
       " 16509,\n",
       " 1055,\n",
       " 18023,\n",
       " 1999,\n",
       " 1996,\n",
       " 2627,\n",
       " 2274,\n",
       " 2086,\n",
       " 1012,\n",
       " 9188,\n",
       " 3393,\n",
       " 3465,\n",
       " 6359,\n",
       " 2005,\n",
       " 6183,\n",
       " 2083,\n",
       " 4121,\n",
       " 3465,\n",
       " 7659,\n",
       " 1999,\n",
       " 3025,\n",
       " 5841,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 4359,\n",
       " 16509,\n",
       " 1055,\n",
       " 8964,\n",
       " 2015,\n",
       " 2011,\n",
       " 2322,\n",
       " 1003,\n",
       " 1998,\n",
       " 21920,\n",
       " 2049,\n",
       " 14877,\n",
       " 2011,\n",
       " 2055,\n",
       " 3263,\n",
       " 2199,\n",
       " 2044,\n",
       " 2635,\n",
       " 3715,\n",
       " 1999,\n",
       " 2639,\n",
       " 1012,\n",
       " 2122,\n",
       " 4506,\n",
       " 3271,\n",
       " 16509,\n",
       " 2735,\n",
       " 1037,\n",
       " 6273,\n",
       " 2549,\n",
       " 24700,\n",
       " 18371,\n",
       " 1006,\n",
       " 1002,\n",
       " 1020,\n",
       " 1012,\n",
       " 1018,\n",
       " 24700,\n",
       " 1007,\n",
       " 3279,\n",
       " 1999,\n",
       " 2456,\n",
       " 2046,\n",
       " 1037,\n",
       " 27533,\n",
       " 24700,\n",
       " 18371,\n",
       " 1006,\n",
       " 1002,\n",
       " 1016,\n",
       " 1012,\n",
       " 1021,\n",
       " 24700,\n",
       " 1007,\n",
       " 5618,\n",
       " 1996,\n",
       " 2206,\n",
       " 2095,\n",
       " 1012,\n",
       " 2076,\n",
       " 2010,\n",
       " 7470,\n",
       " 16509,\n",
       " 2038,\n",
       " 3445,\n",
       " 2049,\n",
       " 3006,\n",
       " 3745,\n",
       " 1998,\n",
       " 2081,\n",
       " 3278,\n",
       " 22215,\n",
       " 1999,\n",
       " 3145,\n",
       " 9167,\n",
       " 6089,\n",
       " 1012,\n",
       " 16509,\n",
       " 8704,\n",
       " 2000,\n",
       " 3623,\n",
       " 4316,\n",
       " 4341,\n",
       " 2000,\n",
       " 2062,\n",
       " 2084,\n",
       " 2176,\n",
       " 2454,\n",
       " 2011,\n",
       " 2263,\n",
       " 12106,\n",
       " 2654,\n",
       " 2047,\n",
       " 4275,\n",
       " 1999,\n",
       " 1996,\n",
       " 2832,\n",
       " 1012,\n",
       " 1999,\n",
       " 2010,\n",
       " 2047,\n",
       " 3105,\n",
       " 2004,\n",
       " 14605,\n",
       " 2708,\n",
       " 3237,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2097,\n",
       " 23313,\n",
       " 2871,\n",
       " 1003,\n",
       " 1997,\n",
       " 2010,\n",
       " 2051,\n",
       " 2000,\n",
       " 14605,\n",
       " 2871,\n",
       " 1003,\n",
       " 2000,\n",
       " 16509,\n",
       " 1998,\n",
       " 1996,\n",
       " 2717,\n",
       " 2000,\n",
       " 1996,\n",
       " 2177,\n",
       " 1055,\n",
       " 3450,\n",
       " 1999,\n",
       " 2167,\n",
       " 2637,\n",
       " 1998,\n",
       " 2060,\n",
       " 3145,\n",
       " 6089,\n",
       " 1012,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2056,\n",
       " 2720,\n",
       " 11895,\n",
       " 3654,\n",
       " 1055,\n",
       " 6098,\n",
       " 2052,\n",
       " 5676,\n",
       " 1037,\n",
       " 25180,\n",
       " 3238,\n",
       " 6653,\n",
       " 1999,\n",
       " 2968,\n",
       " 1012,\n",
       " 1045,\n",
       " 2342,\n",
       " 1037,\n",
       " 4105,\n",
       " 2136,\n",
       " 5214,\n",
       " 1997,\n",
       " 29494,\n",
       " 1996,\n",
       " 2836,\n",
       " 1998,\n",
       " 6959,\n",
       " 1997,\n",
       " 3463,\n",
       " 2008,\n",
       " 2038,\n",
       " 7356,\n",
       " 16509,\n",
       " 2058,\n",
       " 1996,\n",
       " 2627,\n",
       " 2416,\n",
       " 2086,\n",
       " 2720,\n",
       " 1043,\n",
       " 15006,\n",
       " 2078,\n",
       " 2056,\n",
       " 1012,\n",
       " 1045,\n",
       " 2031,\n",
       " 2440,\n",
       " 7023,\n",
       " 1999,\n",
       " 2000,\n",
       " 6182,\n",
       " 19663,\n",
       " 11895,\n",
       " 3654,\n",
       " 1998,\n",
       " 1996,\n",
       " 2047,\n",
       " 4105,\n",
       " 2136,\n",
       " 2000,\n",
       " 2393,\n",
       " 2033,\n",
       " 10408,\n",
       " 1996,\n",
       " 2279,\n",
       " 3127,\n",
       " 1997,\n",
       " 16509,\n",
       " 1055,\n",
       " 3930,\n",
       " 1012,\n",
       " 16509,\n",
       " 2036,\n",
       " 2623,\n",
       " 1037,\n",
       " 2193,\n",
       " 1997,\n",
       " 2060,\n",
       " 2968,\n",
       " 14651,\n",
       " 2007,\n",
       " 15365,\n",
       " 2005,\n",
       " 2195,\n",
       " 3920,\n",
       " 12706,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_ids'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2293, 6207, 2049, 2204, 2005, 2740, 102, 0], [101, 6207, 2003, 21949, 1996, 2773, 2011, 12106, 18059, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['I love apple its good for health','Apple is dominating the word by Launching Iphone'], truncation = True, padding = True  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_new = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,items in vocab_list.items():\n",
    "    dict_new[items] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dict_new.items():\n",
    "    if key in [101, 1045, 2293, 6207, 2049, 2204, 2005, 2740, 102, 0]:\n",
    "        lst_new.append((key,dict_new[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '[PAD]'),\n",
       " (101, '[CLS]'),\n",
       " (102, '[SEP]'),\n",
       " (1045, 'i'),\n",
       " (2005, 'for'),\n",
       " (2049, 'its'),\n",
       " (2204, 'good'),\n",
       " (2293, 'love'),\n",
       " (2740, 'health'),\n",
       " (6207, 'apple')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_new = []\n",
    "for key,value in dict_new.items():\n",
    "    if key in [101, 6207, 2003, 21949, 1996, 2773, 2011, 12106, 18059, 102]:\n",
    "        lst_new.append((key,dict_new[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '[CLS]'),\n",
       " (102, '[SEP]'),\n",
       " (1996, 'the'),\n",
       " (2003, 'is'),\n",
       " (2011, 'by'),\n",
       " (2773, 'word'),\n",
       " (6207, 'apple'),\n",
       " (12106, 'launching'),\n",
       " (18059, 'iphone'),\n",
       " (21949, 'dominating')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[unused0]',\n",
       " '[unused1]',\n",
       " '[unused2]',\n",
       " '[unused3]',\n",
       " '[unused4]',\n",
       " '[unused5]',\n",
       " '[unused6]',\n",
       " '[unused7]',\n",
       " '[unused8]',\n",
       " '[unused9]',\n",
       " '[unused10]',\n",
       " '[unused11]',\n",
       " '[unused12]',\n",
       " '[unused13]',\n",
       " '[unused14]',\n",
       " '[unused15]',\n",
       " '[unused16]',\n",
       " '[unused17]',\n",
       " '[unused18]',\n",
       " '[unused19]',\n",
       " '[unused20]',\n",
       " '[unused21]',\n",
       " '[unused22]',\n",
       " '[unused23]',\n",
       " '[unused24]',\n",
       " '[unused25]',\n",
       " '[unused26]',\n",
       " '[unused27]',\n",
       " '[unused28]',\n",
       " '[unused29]',\n",
       " '[unused30]',\n",
       " '[unused31]',\n",
       " '[unused32]',\n",
       " '[unused33]',\n",
       " '[unused34]',\n",
       " '[unused35]',\n",
       " '[unused36]',\n",
       " '[unused37]',\n",
       " '[unused38]',\n",
       " '[unused39]',\n",
       " '[unused40]',\n",
       " '[unused41]',\n",
       " '[unused42]',\n",
       " '[unused43]',\n",
       " '[unused44]',\n",
       " '[unused45]',\n",
       " '[unused46]',\n",
       " '[unused47]',\n",
       " '[unused48]',\n",
       " '[unused49]',\n",
       " '[unused50]',\n",
       " '[unused51]',\n",
       " '[unused52]',\n",
       " '[unused53]',\n",
       " '[unused54]',\n",
       " '[unused55]',\n",
       " '[unused56]',\n",
       " '[unused57]',\n",
       " '[unused58]',\n",
       " '[unused59]',\n",
       " '[unused60]',\n",
       " '[unused61]',\n",
       " '[unused62]',\n",
       " '[unused63]',\n",
       " '[unused64]',\n",
       " '[unused65]',\n",
       " '[unused66]',\n",
       " '[unused67]',\n",
       " '[unused68]',\n",
       " '[unused69]',\n",
       " '[unused70]',\n",
       " '[unused71]',\n",
       " '[unused72]',\n",
       " '[unused73]',\n",
       " '[unused74]',\n",
       " '[unused75]',\n",
       " '[unused76]',\n",
       " '[unused77]',\n",
       " '[unused78]',\n",
       " '[unused79]',\n",
       " '[unused80]',\n",
       " '[unused81]',\n",
       " '[unused82]',\n",
       " '[unused83]',\n",
       " '[unused84]',\n",
       " '[unused85]',\n",
       " '[unused86]',\n",
       " '[unused87]',\n",
       " '[unused88]',\n",
       " '[unused89]',\n",
       " '[unused90]',\n",
       " '[unused91]',\n",
       " '[unused92]',\n",
       " '[unused93]',\n",
       " '[unused94]',\n",
       " '[unused95]',\n",
       " '[unused96]',\n",
       " '[unused97]',\n",
       " '[unused98]',\n",
       " '[UNK]',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '[unused99]',\n",
       " '[unused100]',\n",
       " '[unused101]',\n",
       " '[unused102]',\n",
       " '[unused103]',\n",
       " '[unused104]',\n",
       " '[unused105]',\n",
       " '[unused106]',\n",
       " '[unused107]',\n",
       " '[unused108]',\n",
       " '[unused109]',\n",
       " '[unused110]',\n",
       " '[unused111]',\n",
       " '[unused112]',\n",
       " '[unused113]',\n",
       " '[unused114]',\n",
       " '[unused115]',\n",
       " '[unused116]',\n",
       " '[unused117]',\n",
       " '[unused118]',\n",
       " '[unused119]',\n",
       " '[unused120]',\n",
       " '[unused121]',\n",
       " '[unused122]',\n",
       " '[unused123]',\n",
       " '[unused124]',\n",
       " '[unused125]',\n",
       " '[unused126]',\n",
       " '[unused127]',\n",
       " '[unused128]',\n",
       " '[unused129]',\n",
       " '[unused130]',\n",
       " '[unused131]',\n",
       " '[unused132]',\n",
       " '[unused133]',\n",
       " '[unused134]',\n",
       " '[unused135]',\n",
       " '[unused136]',\n",
       " '[unused137]',\n",
       " '[unused138]',\n",
       " '[unused139]',\n",
       " '[unused140]',\n",
       " '[unused141]',\n",
       " '[unused142]',\n",
       " '[unused143]',\n",
       " '[unused144]',\n",
       " '[unused145]',\n",
       " '[unused146]',\n",
       " '[unused147]',\n",
       " '[unused148]',\n",
       " '[unused149]',\n",
       " '[unused150]',\n",
       " '[unused151]',\n",
       " '[unused152]',\n",
       " '[unused153]',\n",
       " '[unused154]',\n",
       " '[unused155]',\n",
       " '[unused156]',\n",
       " '[unused157]',\n",
       " '[unused158]',\n",
       " '[unused159]',\n",
       " '[unused160]',\n",
       " '[unused161]',\n",
       " '[unused162]',\n",
       " '[unused163]',\n",
       " '[unused164]',\n",
       " '[unused165]',\n",
       " '[unused166]',\n",
       " '[unused167]',\n",
       " '[unused168]',\n",
       " '[unused169]',\n",
       " '[unused170]',\n",
       " '[unused171]',\n",
       " '[unused172]',\n",
       " '[unused173]',\n",
       " '[unused174]',\n",
       " '[unused175]',\n",
       " '[unused176]',\n",
       " '[unused177]',\n",
       " '[unused178]',\n",
       " '[unused179]',\n",
       " '[unused180]',\n",
       " '[unused181]',\n",
       " '[unused182]',\n",
       " '[unused183]',\n",
       " '[unused184]',\n",
       " '[unused185]',\n",
       " '[unused186]',\n",
       " '[unused187]',\n",
       " '[unused188]',\n",
       " '[unused189]',\n",
       " '[unused190]',\n",
       " '[unused191]',\n",
       " '[unused192]',\n",
       " '[unused193]',\n",
       " '[unused194]',\n",
       " '[unused195]',\n",
       " '[unused196]',\n",
       " '[unused197]',\n",
       " '[unused198]',\n",
       " '[unused199]',\n",
       " '[unused200]',\n",
       " '[unused201]',\n",
       " '[unused202]',\n",
       " '[unused203]',\n",
       " '[unused204]',\n",
       " '[unused205]',\n",
       " '[unused206]',\n",
       " '[unused207]',\n",
       " '[unused208]',\n",
       " '[unused209]',\n",
       " '[unused210]',\n",
       " '[unused211]',\n",
       " '[unused212]',\n",
       " '[unused213]',\n",
       " '[unused214]',\n",
       " '[unused215]',\n",
       " '[unused216]',\n",
       " '[unused217]',\n",
       " '[unused218]',\n",
       " '[unused219]',\n",
       " '[unused220]',\n",
       " '[unused221]',\n",
       " '[unused222]',\n",
       " '[unused223]',\n",
       " '[unused224]',\n",
       " '[unused225]',\n",
       " '[unused226]',\n",
       " '[unused227]',\n",
       " '[unused228]',\n",
       " '[unused229]',\n",
       " '[unused230]',\n",
       " '[unused231]',\n",
       " '[unused232]',\n",
       " '[unused233]',\n",
       " '[unused234]',\n",
       " '[unused235]',\n",
       " '[unused236]',\n",
       " '[unused237]',\n",
       " '[unused238]',\n",
       " '[unused239]',\n",
       " '[unused240]',\n",
       " '[unused241]',\n",
       " '[unused242]',\n",
       " '[unused243]',\n",
       " '[unused244]',\n",
       " '[unused245]',\n",
       " '[unused246]',\n",
       " '[unused247]',\n",
       " '[unused248]',\n",
       " '[unused249]',\n",
       " '[unused250]',\n",
       " '[unused251]',\n",
       " '[unused252]',\n",
       " '[unused253]',\n",
       " '[unused254]',\n",
       " '[unused255]',\n",
       " '[unused256]',\n",
       " '[unused257]',\n",
       " '[unused258]',\n",
       " '[unused259]',\n",
       " '[unused260]',\n",
       " '[unused261]',\n",
       " '[unused262]',\n",
       " '[unused263]',\n",
       " '[unused264]',\n",
       " '[unused265]',\n",
       " '[unused266]',\n",
       " '[unused267]',\n",
       " '[unused268]',\n",
       " '[unused269]',\n",
       " '[unused270]',\n",
       " '[unused271]',\n",
       " '[unused272]',\n",
       " '[unused273]',\n",
       " '[unused274]',\n",
       " '[unused275]',\n",
       " '[unused276]',\n",
       " '[unused277]',\n",
       " '[unused278]',\n",
       " '[unused279]',\n",
       " '[unused280]',\n",
       " '[unused281]',\n",
       " '[unused282]',\n",
       " '[unused283]',\n",
       " '[unused284]',\n",
       " '[unused285]',\n",
       " '[unused286]',\n",
       " '[unused287]',\n",
       " '[unused288]',\n",
       " '[unused289]',\n",
       " '[unused290]',\n",
       " '[unused291]',\n",
       " '[unused292]',\n",
       " '[unused293]',\n",
       " '[unused294]',\n",
       " '[unused295]',\n",
       " '[unused296]',\n",
       " '[unused297]',\n",
       " '[unused298]',\n",
       " '[unused299]',\n",
       " '[unused300]',\n",
       " '[unused301]',\n",
       " '[unused302]',\n",
       " '[unused303]',\n",
       " '[unused304]',\n",
       " '[unused305]',\n",
       " '[unused306]',\n",
       " '[unused307]',\n",
       " '[unused308]',\n",
       " '[unused309]',\n",
       " '[unused310]',\n",
       " '[unused311]',\n",
       " '[unused312]',\n",
       " '[unused313]',\n",
       " '[unused314]',\n",
       " '[unused315]',\n",
       " '[unused316]',\n",
       " '[unused317]',\n",
       " '[unused318]',\n",
       " '[unused319]',\n",
       " '[unused320]',\n",
       " '[unused321]',\n",
       " '[unused322]',\n",
       " '[unused323]',\n",
       " '[unused324]',\n",
       " '[unused325]',\n",
       " '[unused326]',\n",
       " '[unused327]',\n",
       " '[unused328]',\n",
       " '[unused329]',\n",
       " '[unused330]',\n",
       " '[unused331]',\n",
       " '[unused332]',\n",
       " '[unused333]',\n",
       " '[unused334]',\n",
       " '[unused335]',\n",
       " '[unused336]',\n",
       " '[unused337]',\n",
       " '[unused338]',\n",
       " '[unused339]',\n",
       " '[unused340]',\n",
       " '[unused341]',\n",
       " '[unused342]',\n",
       " '[unused343]',\n",
       " '[unused344]',\n",
       " '[unused345]',\n",
       " '[unused346]',\n",
       " '[unused347]',\n",
       " '[unused348]',\n",
       " '[unused349]',\n",
       " '[unused350]',\n",
       " '[unused351]',\n",
       " '[unused352]',\n",
       " '[unused353]',\n",
       " '[unused354]',\n",
       " '[unused355]',\n",
       " '[unused356]',\n",
       " '[unused357]',\n",
       " '[unused358]',\n",
       " '[unused359]',\n",
       " '[unused360]',\n",
       " '[unused361]',\n",
       " '[unused362]',\n",
       " '[unused363]',\n",
       " '[unused364]',\n",
       " '[unused365]',\n",
       " '[unused366]',\n",
       " '[unused367]',\n",
       " '[unused368]',\n",
       " '[unused369]',\n",
       " '[unused370]',\n",
       " '[unused371]',\n",
       " '[unused372]',\n",
       " '[unused373]',\n",
       " '[unused374]',\n",
       " '[unused375]',\n",
       " '[unused376]',\n",
       " '[unused377]',\n",
       " '[unused378]',\n",
       " '[unused379]',\n",
       " '[unused380]',\n",
       " '[unused381]',\n",
       " '[unused382]',\n",
       " '[unused383]',\n",
       " '[unused384]',\n",
       " '[unused385]',\n",
       " '[unused386]',\n",
       " '[unused387]',\n",
       " '[unused388]',\n",
       " '[unused389]',\n",
       " '[unused390]',\n",
       " '[unused391]',\n",
       " '[unused392]',\n",
       " '[unused393]',\n",
       " '[unused394]',\n",
       " '[unused395]',\n",
       " '[unused396]',\n",
       " '[unused397]',\n",
       " '[unused398]',\n",
       " '[unused399]',\n",
       " '[unused400]',\n",
       " '[unused401]',\n",
       " '[unused402]',\n",
       " '[unused403]',\n",
       " '[unused404]',\n",
       " '[unused405]',\n",
       " '[unused406]',\n",
       " '[unused407]',\n",
       " '[unused408]',\n",
       " '[unused409]',\n",
       " '[unused410]',\n",
       " '[unused411]',\n",
       " '[unused412]',\n",
       " '[unused413]',\n",
       " '[unused414]',\n",
       " '[unused415]',\n",
       " '[unused416]',\n",
       " '[unused417]',\n",
       " '[unused418]',\n",
       " '[unused419]',\n",
       " '[unused420]',\n",
       " '[unused421]',\n",
       " '[unused422]',\n",
       " '[unused423]',\n",
       " '[unused424]',\n",
       " '[unused425]',\n",
       " '[unused426]',\n",
       " '[unused427]',\n",
       " '[unused428]',\n",
       " '[unused429]',\n",
       " '[unused430]',\n",
       " '[unused431]',\n",
       " '[unused432]',\n",
       " '[unused433]',\n",
       " '[unused434]',\n",
       " '[unused435]',\n",
       " '[unused436]',\n",
       " '[unused437]',\n",
       " '[unused438]',\n",
       " '[unused439]',\n",
       " '[unused440]',\n",
       " '[unused441]',\n",
       " '[unused442]',\n",
       " '[unused443]',\n",
       " '[unused444]',\n",
       " '[unused445]',\n",
       " '[unused446]',\n",
       " '[unused447]',\n",
       " '[unused448]',\n",
       " '[unused449]',\n",
       " '[unused450]',\n",
       " '[unused451]',\n",
       " '[unused452]',\n",
       " '[unused453]',\n",
       " '[unused454]',\n",
       " '[unused455]',\n",
       " '[unused456]',\n",
       " '[unused457]',\n",
       " '[unused458]',\n",
       " '[unused459]',\n",
       " '[unused460]',\n",
       " '[unused461]',\n",
       " '[unused462]',\n",
       " '[unused463]',\n",
       " '[unused464]',\n",
       " '[unused465]',\n",
       " '[unused466]',\n",
       " '[unused467]',\n",
       " '[unused468]',\n",
       " '[unused469]',\n",
       " '[unused470]',\n",
       " '[unused471]',\n",
       " '[unused472]',\n",
       " '[unused473]',\n",
       " '[unused474]',\n",
       " '[unused475]',\n",
       " '[unused476]',\n",
       " '[unused477]',\n",
       " '[unused478]',\n",
       " '[unused479]',\n",
       " '[unused480]',\n",
       " '[unused481]',\n",
       " '[unused482]',\n",
       " '[unused483]',\n",
       " '[unused484]',\n",
       " '[unused485]',\n",
       " '[unused486]',\n",
       " '[unused487]',\n",
       " '[unused488]',\n",
       " '[unused489]',\n",
       " '[unused490]',\n",
       " '[unused491]',\n",
       " '[unused492]',\n",
       " '[unused493]',\n",
       " '[unused494]',\n",
       " '[unused495]',\n",
       " '[unused496]',\n",
       " '[unused497]',\n",
       " '[unused498]',\n",
       " '[unused499]',\n",
       " '[unused500]',\n",
       " '[unused501]',\n",
       " '[unused502]',\n",
       " '[unused503]',\n",
       " '[unused504]',\n",
       " '[unused505]',\n",
       " '[unused506]',\n",
       " '[unused507]',\n",
       " '[unused508]',\n",
       " '[unused509]',\n",
       " '[unused510]',\n",
       " '[unused511]',\n",
       " '[unused512]',\n",
       " '[unused513]',\n",
       " '[unused514]',\n",
       " '[unused515]',\n",
       " '[unused516]',\n",
       " '[unused517]',\n",
       " '[unused518]',\n",
       " '[unused519]',\n",
       " '[unused520]',\n",
       " '[unused521]',\n",
       " '[unused522]',\n",
       " '[unused523]',\n",
       " '[unused524]',\n",
       " '[unused525]',\n",
       " '[unused526]',\n",
       " '[unused527]',\n",
       " '[unused528]',\n",
       " '[unused529]',\n",
       " '[unused530]',\n",
       " '[unused531]',\n",
       " '[unused532]',\n",
       " '[unused533]',\n",
       " '[unused534]',\n",
       " '[unused535]',\n",
       " '[unused536]',\n",
       " '[unused537]',\n",
       " '[unused538]',\n",
       " '[unused539]',\n",
       " '[unused540]',\n",
       " '[unused541]',\n",
       " '[unused542]',\n",
       " '[unused543]',\n",
       " '[unused544]',\n",
       " '[unused545]',\n",
       " '[unused546]',\n",
       " '[unused547]',\n",
       " '[unused548]',\n",
       " '[unused549]',\n",
       " '[unused550]',\n",
       " '[unused551]',\n",
       " '[unused552]',\n",
       " '[unused553]',\n",
       " '[unused554]',\n",
       " '[unused555]',\n",
       " '[unused556]',\n",
       " '[unused557]',\n",
       " '[unused558]',\n",
       " '[unused559]',\n",
       " '[unused560]',\n",
       " '[unused561]',\n",
       " '[unused562]',\n",
       " '[unused563]',\n",
       " '[unused564]',\n",
       " '[unused565]',\n",
       " '[unused566]',\n",
       " '[unused567]',\n",
       " '[unused568]',\n",
       " '[unused569]',\n",
       " '[unused570]',\n",
       " '[unused571]',\n",
       " '[unused572]',\n",
       " '[unused573]',\n",
       " '[unused574]',\n",
       " '[unused575]',\n",
       " '[unused576]',\n",
       " '[unused577]',\n",
       " '[unused578]',\n",
       " '[unused579]',\n",
       " '[unused580]',\n",
       " '[unused581]',\n",
       " '[unused582]',\n",
       " '[unused583]',\n",
       " '[unused584]',\n",
       " '[unused585]',\n",
       " '[unused586]',\n",
       " '[unused587]',\n",
       " '[unused588]',\n",
       " '[unused589]',\n",
       " '[unused590]',\n",
       " '[unused591]',\n",
       " '[unused592]',\n",
       " '[unused593]',\n",
       " '[unused594]',\n",
       " '[unused595]',\n",
       " '[unused596]',\n",
       " '[unused597]',\n",
       " '[unused598]',\n",
       " '[unused599]',\n",
       " '[unused600]',\n",
       " '[unused601]',\n",
       " '[unused602]',\n",
       " '[unused603]',\n",
       " '[unused604]',\n",
       " '[unused605]',\n",
       " '[unused606]',\n",
       " '[unused607]',\n",
       " '[unused608]',\n",
       " '[unused609]',\n",
       " '[unused610]',\n",
       " '[unused611]',\n",
       " '[unused612]',\n",
       " '[unused613]',\n",
       " '[unused614]',\n",
       " '[unused615]',\n",
       " '[unused616]',\n",
       " '[unused617]',\n",
       " '[unused618]',\n",
       " '[unused619]',\n",
       " '[unused620]',\n",
       " '[unused621]',\n",
       " '[unused622]',\n",
       " '[unused623]',\n",
       " '[unused624]',\n",
       " '[unused625]',\n",
       " '[unused626]',\n",
       " '[unused627]',\n",
       " '[unused628]',\n",
       " '[unused629]',\n",
       " '[unused630]',\n",
       " '[unused631]',\n",
       " '[unused632]',\n",
       " '[unused633]',\n",
       " '[unused634]',\n",
       " '[unused635]',\n",
       " '[unused636]',\n",
       " '[unused637]',\n",
       " '[unused638]',\n",
       " '[unused639]',\n",
       " '[unused640]',\n",
       " '[unused641]',\n",
       " '[unused642]',\n",
       " '[unused643]',\n",
       " '[unused644]',\n",
       " '[unused645]',\n",
       " '[unused646]',\n",
       " '[unused647]',\n",
       " '[unused648]',\n",
       " '[unused649]',\n",
       " '[unused650]',\n",
       " '[unused651]',\n",
       " '[unused652]',\n",
       " '[unused653]',\n",
       " '[unused654]',\n",
       " '[unused655]',\n",
       " '[unused656]',\n",
       " '[unused657]',\n",
       " '[unused658]',\n",
       " '[unused659]',\n",
       " '[unused660]',\n",
       " '[unused661]',\n",
       " '[unused662]',\n",
       " '[unused663]',\n",
       " '[unused664]',\n",
       " '[unused665]',\n",
       " '[unused666]',\n",
       " '[unused667]',\n",
       " '[unused668]',\n",
       " '[unused669]',\n",
       " '[unused670]',\n",
       " '[unused671]',\n",
       " '[unused672]',\n",
       " '[unused673]',\n",
       " '[unused674]',\n",
       " '[unused675]',\n",
       " '[unused676]',\n",
       " '[unused677]',\n",
       " '[unused678]',\n",
       " '[unused679]',\n",
       " '[unused680]',\n",
       " '[unused681]',\n",
       " '[unused682]',\n",
       " '[unused683]',\n",
       " '[unused684]',\n",
       " '[unused685]',\n",
       " '[unused686]',\n",
       " '[unused687]',\n",
       " '[unused688]',\n",
       " '[unused689]',\n",
       " '[unused690]',\n",
       " '[unused691]',\n",
       " '[unused692]',\n",
       " '[unused693]',\n",
       " '[unused694]',\n",
       " '[unused695]',\n",
       " '[unused696]',\n",
       " '[unused697]',\n",
       " '[unused698]',\n",
       " '[unused699]',\n",
       " '[unused700]',\n",
       " '[unused701]',\n",
       " '[unused702]',\n",
       " '[unused703]',\n",
       " '[unused704]',\n",
       " '[unused705]',\n",
       " '[unused706]',\n",
       " '[unused707]',\n",
       " '[unused708]',\n",
       " '[unused709]',\n",
       " '[unused710]',\n",
       " '[unused711]',\n",
       " '[unused712]',\n",
       " '[unused713]',\n",
       " '[unused714]',\n",
       " '[unused715]',\n",
       " '[unused716]',\n",
       " '[unused717]',\n",
       " '[unused718]',\n",
       " '[unused719]',\n",
       " '[unused720]',\n",
       " '[unused721]',\n",
       " '[unused722]',\n",
       " '[unused723]',\n",
       " '[unused724]',\n",
       " '[unused725]',\n",
       " '[unused726]',\n",
       " '[unused727]',\n",
       " '[unused728]',\n",
       " '[unused729]',\n",
       " '[unused730]',\n",
       " '[unused731]',\n",
       " '[unused732]',\n",
       " '[unused733]',\n",
       " '[unused734]',\n",
       " '[unused735]',\n",
       " '[unused736]',\n",
       " '[unused737]',\n",
       " '[unused738]',\n",
       " '[unused739]',\n",
       " '[unused740]',\n",
       " '[unused741]',\n",
       " '[unused742]',\n",
       " '[unused743]',\n",
       " '[unused744]',\n",
       " '[unused745]',\n",
       " '[unused746]',\n",
       " '[unused747]',\n",
       " '[unused748]',\n",
       " '[unused749]',\n",
       " '[unused750]',\n",
       " '[unused751]',\n",
       " '[unused752]',\n",
       " '[unused753]',\n",
       " '[unused754]',\n",
       " '[unused755]',\n",
       " '[unused756]',\n",
       " '[unused757]',\n",
       " '[unused758]',\n",
       " '[unused759]',\n",
       " '[unused760]',\n",
       " '[unused761]',\n",
       " '[unused762]',\n",
       " '[unused763]',\n",
       " '[unused764]',\n",
       " '[unused765]',\n",
       " '[unused766]',\n",
       " '[unused767]',\n",
       " '[unused768]',\n",
       " '[unused769]',\n",
       " '[unused770]',\n",
       " '[unused771]',\n",
       " '[unused772]',\n",
       " '[unused773]',\n",
       " '[unused774]',\n",
       " '[unused775]',\n",
       " '[unused776]',\n",
       " '[unused777]',\n",
       " '[unused778]',\n",
       " '[unused779]',\n",
       " '[unused780]',\n",
       " '[unused781]',\n",
       " '[unused782]',\n",
       " '[unused783]',\n",
       " '[unused784]',\n",
       " '[unused785]',\n",
       " '[unused786]',\n",
       " '[unused787]',\n",
       " '[unused788]',\n",
       " '[unused789]',\n",
       " '[unused790]',\n",
       " '[unused791]',\n",
       " '[unused792]',\n",
       " '[unused793]',\n",
       " '[unused794]',\n",
       " '[unused795]',\n",
       " '[unused796]',\n",
       " '[unused797]',\n",
       " '[unused798]',\n",
       " '[unused799]',\n",
       " '[unused800]',\n",
       " '[unused801]',\n",
       " '[unused802]',\n",
       " '[unused803]',\n",
       " '[unused804]',\n",
       " '[unused805]',\n",
       " '[unused806]',\n",
       " '[unused807]',\n",
       " '[unused808]',\n",
       " '[unused809]',\n",
       " '[unused810]',\n",
       " '[unused811]',\n",
       " '[unused812]',\n",
       " '[unused813]',\n",
       " '[unused814]',\n",
       " '[unused815]',\n",
       " '[unused816]',\n",
       " '[unused817]',\n",
       " '[unused818]',\n",
       " '[unused819]',\n",
       " '[unused820]',\n",
       " '[unused821]',\n",
       " '[unused822]',\n",
       " '[unused823]',\n",
       " '[unused824]',\n",
       " '[unused825]',\n",
       " '[unused826]',\n",
       " '[unused827]',\n",
       " '[unused828]',\n",
       " '[unused829]',\n",
       " '[unused830]',\n",
       " '[unused831]',\n",
       " '[unused832]',\n",
       " '[unused833]',\n",
       " '[unused834]',\n",
       " '[unused835]',\n",
       " '[unused836]',\n",
       " '[unused837]',\n",
       " '[unused838]',\n",
       " '[unused839]',\n",
       " '[unused840]',\n",
       " '[unused841]',\n",
       " '[unused842]',\n",
       " '[unused843]',\n",
       " '[unused844]',\n",
       " '[unused845]',\n",
       " '[unused846]',\n",
       " '[unused847]',\n",
       " '[unused848]',\n",
       " '[unused849]',\n",
       " '[unused850]',\n",
       " '[unused851]',\n",
       " '[unused852]',\n",
       " '[unused853]',\n",
       " '[unused854]',\n",
       " '[unused855]',\n",
       " '[unused856]',\n",
       " '[unused857]',\n",
       " '[unused858]',\n",
       " '[unused859]',\n",
       " '[unused860]',\n",
       " '[unused861]',\n",
       " '[unused862]',\n",
       " '[unused863]',\n",
       " '[unused864]',\n",
       " '[unused865]',\n",
       " '[unused866]',\n",
       " '[unused867]',\n",
       " '[unused868]',\n",
       " '[unused869]',\n",
       " '[unused870]',\n",
       " '[unused871]',\n",
       " '[unused872]',\n",
       " '[unused873]',\n",
       " '[unused874]',\n",
       " '[unused875]',\n",
       " '[unused876]',\n",
       " '[unused877]',\n",
       " '[unused878]',\n",
       " '[unused879]',\n",
       " '[unused880]',\n",
       " '[unused881]',\n",
       " '[unused882]',\n",
       " '[unused883]',\n",
       " '[unused884]',\n",
       " '[unused885]',\n",
       " '[unused886]',\n",
       " '[unused887]',\n",
       " '[unused888]',\n",
       " '[unused889]',\n",
       " '[unused890]',\n",
       " '[unused891]',\n",
       " '[unused892]',\n",
       " '[unused893]',\n",
       " '[unused894]',\n",
       " '[unused895]',\n",
       " '[unused896]',\n",
       " '[unused897]',\n",
       " '[unused898]',\n",
       " '[unused899]',\n",
       " '[unused900]',\n",
       " '[unused901]',\n",
       " '[unused902]',\n",
       " '[unused903]',\n",
       " '[unused904]',\n",
       " '[unused905]',\n",
       " '[unused906]',\n",
       " '[unused907]',\n",
       " '[unused908]',\n",
       " '[unused909]',\n",
       " '[unused910]',\n",
       " '[unused911]',\n",
       " '[unused912]',\n",
       " '[unused913]',\n",
       " '[unused914]',\n",
       " '[unused915]',\n",
       " '[unused916]',\n",
       " '[unused917]',\n",
       " '[unused918]',\n",
       " '[unused919]',\n",
       " '[unused920]',\n",
       " '[unused921]',\n",
       " '[unused922]',\n",
       " '[unused923]',\n",
       " '[unused924]',\n",
       " '[unused925]',\n",
       " '[unused926]',\n",
       " '[unused927]',\n",
       " '[unused928]',\n",
       " '[unused929]',\n",
       " '[unused930]',\n",
       " '[unused931]',\n",
       " '[unused932]',\n",
       " '[unused933]',\n",
       " '[unused934]',\n",
       " '[unused935]',\n",
       " '[unused936]',\n",
       " '[unused937]',\n",
       " '[unused938]',\n",
       " '[unused939]',\n",
       " '[unused940]',\n",
       " '[unused941]',\n",
       " '[unused942]',\n",
       " '[unused943]',\n",
       " '[unused944]',\n",
       " '[unused945]',\n",
       " '[unused946]',\n",
       " '[unused947]',\n",
       " '[unused948]',\n",
       " '[unused949]',\n",
       " '[unused950]',\n",
       " '[unused951]',\n",
       " '[unused952]',\n",
       " '[unused953]',\n",
       " '[unused954]',\n",
       " '[unused955]',\n",
       " '[unused956]',\n",
       " '[unused957]',\n",
       " '[unused958]',\n",
       " '[unused959]',\n",
       " '[unused960]',\n",
       " '[unused961]',\n",
       " '[unused962]',\n",
       " '[unused963]',\n",
       " '[unused964]',\n",
       " '[unused965]',\n",
       " '[unused966]',\n",
       " '[unused967]',\n",
       " '[unused968]',\n",
       " '[unused969]',\n",
       " '[unused970]',\n",
       " '[unused971]',\n",
       " '[unused972]',\n",
       " '[unused973]',\n",
       " '[unused974]',\n",
       " '[unused975]',\n",
       " '[unused976]',\n",
       " '[unused977]',\n",
       " '[unused978]',\n",
       " '[unused979]',\n",
       " '[unused980]',\n",
       " '[unused981]',\n",
       " '[unused982]',\n",
       " '[unused983]',\n",
       " '[unused984]',\n",
       " '[unused985]',\n",
       " '[unused986]',\n",
       " '[unused987]',\n",
       " '[unused988]',\n",
       " '[unused989]',\n",
       " '[unused990]',\n",
       " '[unused991]',\n",
       " '[unused992]',\n",
       " '[unused993]',\n",
       " '!',\n",
       " ...]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(vocab_list.keys())\n",
    "vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[101, 6207, 2003, 21949, 1996, 2773, 2011, 12106, 18059, 102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.064195</td>\n",
       "      <td>-0.132760</td>\n",
       "      <td>0.063612</td>\n",
       "      <td>-0.240925</td>\n",
       "      <td>-0.083574</td>\n",
       "      <td>-0.134110</td>\n",
       "      <td>0.183394</td>\n",
       "      <td>0.636293</td>\n",
       "      <td>-0.207520</td>\n",
       "      <td>-0.309251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.064992</td>\n",
       "      <td>0.166451</td>\n",
       "      <td>0.129627</td>\n",
       "      <td>0.382242</td>\n",
       "      <td>-0.049030</td>\n",
       "      <td>0.070702</td>\n",
       "      <td>-0.305815</td>\n",
       "      <td>0.402528</td>\n",
       "      <td>0.275772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.220875</td>\n",
       "      <td>0.536741</td>\n",
       "      <td>0.062352</td>\n",
       "      <td>-0.366541</td>\n",
       "      <td>0.962489</td>\n",
       "      <td>0.159533</td>\n",
       "      <td>0.142841</td>\n",
       "      <td>0.594624</td>\n",
       "      <td>0.065557</td>\n",
       "      <td>-0.220243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327927</td>\n",
       "      <td>-0.237269</td>\n",
       "      <td>0.202416</td>\n",
       "      <td>0.080851</td>\n",
       "      <td>0.609687</td>\n",
       "      <td>0.311438</td>\n",
       "      <td>0.066710</td>\n",
       "      <td>-0.261410</td>\n",
       "      <td>0.278812</td>\n",
       "      <td>-0.386621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.165870</td>\n",
       "      <td>-0.147410</td>\n",
       "      <td>0.026216</td>\n",
       "      <td>-0.123433</td>\n",
       "      <td>0.208637</td>\n",
       "      <td>-0.286534</td>\n",
       "      <td>-0.077499</td>\n",
       "      <td>1.027212</td>\n",
       "      <td>-0.271760</td>\n",
       "      <td>-0.336569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228105</td>\n",
       "      <td>-0.242532</td>\n",
       "      <td>0.012316</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>0.203882</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.152664</td>\n",
       "      <td>-0.380540</td>\n",
       "      <td>0.382057</td>\n",
       "      <td>0.248115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.171084</td>\n",
       "      <td>-0.123699</td>\n",
       "      <td>0.054632</td>\n",
       "      <td>-0.128233</td>\n",
       "      <td>-0.145563</td>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.031279</td>\n",
       "      <td>0.706398</td>\n",
       "      <td>-0.430838</td>\n",
       "      <td>-0.281623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102147</td>\n",
       "      <td>0.348230</td>\n",
       "      <td>0.138830</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>0.091121</td>\n",
       "      <td>0.054534</td>\n",
       "      <td>0.321404</td>\n",
       "      <td>-0.297334</td>\n",
       "      <td>0.050124</td>\n",
       "      <td>-0.073259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222836</td>\n",
       "      <td>-0.070597</td>\n",
       "      <td>0.223148</td>\n",
       "      <td>0.135712</td>\n",
       "      <td>-0.075137</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.270980</td>\n",
       "      <td>1.035562</td>\n",
       "      <td>-0.531304</td>\n",
       "      <td>-0.333751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255538</td>\n",
       "      <td>0.589822</td>\n",
       "      <td>0.187245</td>\n",
       "      <td>-0.139378</td>\n",
       "      <td>0.406396</td>\n",
       "      <td>-0.099877</td>\n",
       "      <td>-0.093794</td>\n",
       "      <td>-0.184244</td>\n",
       "      <td>0.451584</td>\n",
       "      <td>0.220263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.179680</td>\n",
       "      <td>-0.130723</td>\n",
       "      <td>0.388627</td>\n",
       "      <td>-0.103329</td>\n",
       "      <td>-0.075229</td>\n",
       "      <td>0.287989</td>\n",
       "      <td>0.413951</td>\n",
       "      <td>1.124503</td>\n",
       "      <td>-0.075223</td>\n",
       "      <td>-0.226912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062560</td>\n",
       "      <td>0.416276</td>\n",
       "      <td>0.420572</td>\n",
       "      <td>-0.114803</td>\n",
       "      <td>0.497528</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>0.147104</td>\n",
       "      <td>-0.397269</td>\n",
       "      <td>0.220228</td>\n",
       "      <td>0.082987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022870</td>\n",
       "      <td>-0.153145</td>\n",
       "      <td>0.301243</td>\n",
       "      <td>-0.380410</td>\n",
       "      <td>-0.058848</td>\n",
       "      <td>-0.448745</td>\n",
       "      <td>-0.062261</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>-0.349961</td>\n",
       "      <td>-0.497509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089242</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>-0.280267</td>\n",
       "      <td>0.343526</td>\n",
       "      <td>0.082598</td>\n",
       "      <td>0.034074</td>\n",
       "      <td>-0.032630</td>\n",
       "      <td>-0.426993</td>\n",
       "      <td>0.360913</td>\n",
       "      <td>0.514137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.058128</td>\n",
       "      <td>0.148258</td>\n",
       "      <td>0.203097</td>\n",
       "      <td>-0.261881</td>\n",
       "      <td>0.211331</td>\n",
       "      <td>-0.327688</td>\n",
       "      <td>-0.082414</td>\n",
       "      <td>0.771493</td>\n",
       "      <td>-0.206124</td>\n",
       "      <td>-0.089921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081577</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>-0.102073</td>\n",
       "      <td>0.241130</td>\n",
       "      <td>0.485556</td>\n",
       "      <td>-0.087771</td>\n",
       "      <td>0.254292</td>\n",
       "      <td>-0.464162</td>\n",
       "      <td>0.491040</td>\n",
       "      <td>-0.173747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.195728</td>\n",
       "      <td>0.179158</td>\n",
       "      <td>0.149608</td>\n",
       "      <td>-0.369451</td>\n",
       "      <td>0.789648</td>\n",
       "      <td>0.279344</td>\n",
       "      <td>-0.065213</td>\n",
       "      <td>0.526035</td>\n",
       "      <td>-0.050126</td>\n",
       "      <td>-0.443934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065183</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.128338</td>\n",
       "      <td>-0.035017</td>\n",
       "      <td>0.583454</td>\n",
       "      <td>0.185133</td>\n",
       "      <td>0.262193</td>\n",
       "      <td>-0.176383</td>\n",
       "      <td>0.069893</td>\n",
       "      <td>-0.561593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.767364</td>\n",
       "      <td>0.404493</td>\n",
       "      <td>-0.399893</td>\n",
       "      <td>0.322853</td>\n",
       "      <td>-0.500069</td>\n",
       "      <td>-0.933793</td>\n",
       "      <td>0.491240</td>\n",
       "      <td>-0.326429</td>\n",
       "      <td>0.551198</td>\n",
       "      <td>0.081350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032090</td>\n",
       "      <td>0.111364</td>\n",
       "      <td>-0.357744</td>\n",
       "      <td>-0.665155</td>\n",
       "      <td>0.461960</td>\n",
       "      <td>-1.010690</td>\n",
       "      <td>-0.164280</td>\n",
       "      <td>-0.249979</td>\n",
       "      <td>-0.460849</td>\n",
       "      <td>-0.531588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.064195 -0.132760  0.063612 -0.240925 -0.083574 -0.134110  0.183394   \n",
       "1  0.220875  0.536741  0.062352 -0.366541  0.962489  0.159533  0.142841   \n",
       "2 -0.165870 -0.147410  0.026216 -0.123433  0.208637 -0.286534 -0.077499   \n",
       "3  0.171084 -0.123699  0.054632 -0.128233 -0.145563  0.028687  0.031279   \n",
       "4  0.222836 -0.070597  0.223148  0.135712 -0.075137  0.105200  0.270980   \n",
       "5  0.179680 -0.130723  0.388627 -0.103329 -0.075229  0.287989  0.413951   \n",
       "6  0.022870 -0.153145  0.301243 -0.380410 -0.058848 -0.448745 -0.062261   \n",
       "7 -0.058128  0.148258  0.203097 -0.261881  0.211331 -0.327688 -0.082414   \n",
       "8 -0.195728  0.179158  0.149608 -0.369451  0.789648  0.279344 -0.065213   \n",
       "9  0.767364  0.404493 -0.399893  0.322853 -0.500069 -0.933793  0.491240   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.636293 -0.207520 -0.309251  ...  0.066120  0.064992  0.166451  0.129627   \n",
       "1  0.594624  0.065557 -0.220243  ... -0.327927 -0.237269  0.202416  0.080851   \n",
       "2  1.027212 -0.271760 -0.336569  ...  0.228105 -0.242532  0.012316  0.193832   \n",
       "3  0.706398 -0.430838 -0.281623  ... -0.102147  0.348230  0.138830 -0.017375   \n",
       "4  1.035562 -0.531304 -0.333751  ...  0.255538  0.589822  0.187245 -0.139378   \n",
       "5  1.124503 -0.075223 -0.226912  ...  0.062560  0.416276  0.420572 -0.114803   \n",
       "6  0.835166 -0.349961 -0.497509  ...  0.089242  0.092704 -0.280267  0.343526   \n",
       "7  0.771493 -0.206124 -0.089921  ... -0.081577  0.188636 -0.102073  0.241130   \n",
       "8  0.526035 -0.050126 -0.443934  ...  0.065183 -0.000770 -0.128338 -0.035017   \n",
       "9 -0.326429  0.551198  0.081350  ...  0.032090  0.111364 -0.357744 -0.665155   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.382242 -0.049030  0.070702 -0.305815  0.402528  0.275772  \n",
       "1  0.609687  0.311438  0.066710 -0.261410  0.278812 -0.386621  \n",
       "2  0.203882  0.195478  0.152664 -0.380540  0.382057  0.248115  \n",
       "3  0.091121  0.054534  0.321404 -0.297334  0.050124 -0.073259  \n",
       "4  0.406396 -0.099877 -0.093794 -0.184244  0.451584  0.220263  \n",
       "5  0.497528  0.034608  0.147104 -0.397269  0.220228  0.082987  \n",
       "6  0.082598  0.034074 -0.032630 -0.426993  0.360913  0.514137  \n",
       "7  0.485556 -0.087771  0.254292 -0.464162  0.491040 -0.173747  \n",
       "8  0.583454  0.185133  0.262193 -0.176383  0.069893 -0.561593  \n",
       "9  0.461960 -1.010690 -0.164280 -0.249979 -0.460849 -0.531588  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(embeddings[0])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.056091</td>\n",
       "      <td>0.127475</td>\n",
       "      <td>0.056981</td>\n",
       "      <td>-0.148942</td>\n",
       "      <td>-0.040867</td>\n",
       "      <td>0.033770</td>\n",
       "      <td>0.171286</td>\n",
       "      <td>0.548885</td>\n",
       "      <td>-0.201742</td>\n",
       "      <td>-0.213859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002993</td>\n",
       "      <td>-0.205541</td>\n",
       "      <td>-0.012523</td>\n",
       "      <td>0.082158</td>\n",
       "      <td>0.326329</td>\n",
       "      <td>-0.072332</td>\n",
       "      <td>-0.082064</td>\n",
       "      <td>-0.032882</td>\n",
       "      <td>0.219267</td>\n",
       "      <td>0.308177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.393668</td>\n",
       "      <td>0.208844</td>\n",
       "      <td>-0.278210</td>\n",
       "      <td>0.337384</td>\n",
       "      <td>0.406747</td>\n",
       "      <td>0.074446</td>\n",
       "      <td>1.157242</td>\n",
       "      <td>0.051203</td>\n",
       "      <td>-0.038894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616548</td>\n",
       "      <td>-0.129436</td>\n",
       "      <td>0.274405</td>\n",
       "      <td>-0.054965</td>\n",
       "      <td>0.153705</td>\n",
       "      <td>-0.019826</td>\n",
       "      <td>-0.099569</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.482723</td>\n",
       "      <td>0.426559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.905401</td>\n",
       "      <td>0.705414</td>\n",
       "      <td>0.704874</td>\n",
       "      <td>0.045916</td>\n",
       "      <td>0.303495</td>\n",
       "      <td>0.122623</td>\n",
       "      <td>-0.097725</td>\n",
       "      <td>0.961187</td>\n",
       "      <td>-0.148644</td>\n",
       "      <td>-0.152075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142680</td>\n",
       "      <td>-0.051404</td>\n",
       "      <td>0.089684</td>\n",
       "      <td>0.158385</td>\n",
       "      <td>0.107652</td>\n",
       "      <td>0.149618</td>\n",
       "      <td>-0.112709</td>\n",
       "      <td>-0.148615</td>\n",
       "      <td>0.242422</td>\n",
       "      <td>0.218383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065937</td>\n",
       "      <td>0.251994</td>\n",
       "      <td>-0.061540</td>\n",
       "      <td>0.082982</td>\n",
       "      <td>0.716709</td>\n",
       "      <td>0.245081</td>\n",
       "      <td>-0.027020</td>\n",
       "      <td>0.797194</td>\n",
       "      <td>-0.059649</td>\n",
       "      <td>-0.090892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369977</td>\n",
       "      <td>-0.089909</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.160863</td>\n",
       "      <td>0.310937</td>\n",
       "      <td>0.184556</td>\n",
       "      <td>0.321837</td>\n",
       "      <td>0.152796</td>\n",
       "      <td>-0.160329</td>\n",
       "      <td>-0.388230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.124857</td>\n",
       "      <td>0.291590</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>0.048759</td>\n",
       "      <td>0.028038</td>\n",
       "      <td>0.116337</td>\n",
       "      <td>0.241820</td>\n",
       "      <td>1.246581</td>\n",
       "      <td>-0.061237</td>\n",
       "      <td>-0.396730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037526</td>\n",
       "      <td>-0.076632</td>\n",
       "      <td>0.182639</td>\n",
       "      <td>0.316800</td>\n",
       "      <td>-0.147912</td>\n",
       "      <td>-0.252124</td>\n",
       "      <td>0.332141</td>\n",
       "      <td>0.135236</td>\n",
       "      <td>-0.096692</td>\n",
       "      <td>0.606513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.046862</td>\n",
       "      <td>0.306907</td>\n",
       "      <td>0.426648</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>0.431170</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.171610</td>\n",
       "      <td>0.937630</td>\n",
       "      <td>-0.199760</td>\n",
       "      <td>-0.227127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245337</td>\n",
       "      <td>-0.207044</td>\n",
       "      <td>-0.069029</td>\n",
       "      <td>0.282795</td>\n",
       "      <td>0.022712</td>\n",
       "      <td>-0.071365</td>\n",
       "      <td>-0.044824</td>\n",
       "      <td>-0.075301</td>\n",
       "      <td>-0.041037</td>\n",
       "      <td>-0.442021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.391529</td>\n",
       "      <td>0.097367</td>\n",
       "      <td>0.374286</td>\n",
       "      <td>0.176169</td>\n",
       "      <td>0.195926</td>\n",
       "      <td>0.256752</td>\n",
       "      <td>0.042226</td>\n",
       "      <td>0.846141</td>\n",
       "      <td>-0.379775</td>\n",
       "      <td>-0.527885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044587</td>\n",
       "      <td>-0.262445</td>\n",
       "      <td>0.098442</td>\n",
       "      <td>0.229352</td>\n",
       "      <td>0.172118</td>\n",
       "      <td>-0.099624</td>\n",
       "      <td>-0.245188</td>\n",
       "      <td>0.074528</td>\n",
       "      <td>0.212070</td>\n",
       "      <td>-0.181547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.149265</td>\n",
       "      <td>-0.153312</td>\n",
       "      <td>0.042436</td>\n",
       "      <td>0.072645</td>\n",
       "      <td>0.443485</td>\n",
       "      <td>0.152356</td>\n",
       "      <td>0.034776</td>\n",
       "      <td>1.067249</td>\n",
       "      <td>-0.456926</td>\n",
       "      <td>-0.354284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193140</td>\n",
       "      <td>-0.259188</td>\n",
       "      <td>0.159221</td>\n",
       "      <td>0.124030</td>\n",
       "      <td>0.466674</td>\n",
       "      <td>-0.151697</td>\n",
       "      <td>0.202071</td>\n",
       "      <td>0.132119</td>\n",
       "      <td>0.038344</td>\n",
       "      <td>-0.406034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.953832</td>\n",
       "      <td>0.421346</td>\n",
       "      <td>-0.377084</td>\n",
       "      <td>0.530712</td>\n",
       "      <td>-0.459511</td>\n",
       "      <td>-0.722913</td>\n",
       "      <td>0.210109</td>\n",
       "      <td>-0.135080</td>\n",
       "      <td>0.517462</td>\n",
       "      <td>0.128379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103504</td>\n",
       "      <td>0.085531</td>\n",
       "      <td>-0.348118</td>\n",
       "      <td>-0.659088</td>\n",
       "      <td>0.258466</td>\n",
       "      <td>-0.688506</td>\n",
       "      <td>0.116538</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>-0.563591</td>\n",
       "      <td>-0.330741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145254</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>0.069890</td>\n",
       "      <td>0.164813</td>\n",
       "      <td>0.182884</td>\n",
       "      <td>0.187455</td>\n",
       "      <td>0.032175</td>\n",
       "      <td>0.383692</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>-0.316191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092373</td>\n",
       "      <td>-0.090855</td>\n",
       "      <td>-0.233686</td>\n",
       "      <td>0.122942</td>\n",
       "      <td>0.048451</td>\n",
       "      <td>-0.159634</td>\n",
       "      <td>0.078881</td>\n",
       "      <td>0.169858</td>\n",
       "      <td>-0.008573</td>\n",
       "      <td>0.105760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.056091  0.127475  0.056981 -0.148942 -0.040867  0.033770  0.171286   \n",
       "1  0.388158  0.393668  0.208844 -0.278210  0.337384  0.406747  0.074446   \n",
       "2  0.905401  0.705414  0.704874  0.045916  0.303495  0.122623 -0.097725   \n",
       "3  0.065937  0.251994 -0.061540  0.082982  0.716709  0.245081 -0.027020   \n",
       "4 -0.124857  0.291590  0.325362  0.048759  0.028038  0.116337  0.241820   \n",
       "5 -0.046862  0.306907  0.426648 -0.038459  0.431170  0.022300  0.171610   \n",
       "6  0.391529  0.097367  0.374286  0.176169  0.195926  0.256752  0.042226   \n",
       "7  0.149265 -0.153312  0.042436  0.072645  0.443485  0.152356  0.034776   \n",
       "8  0.953832  0.421346 -0.377084  0.530712 -0.459511 -0.722913  0.210109   \n",
       "9  0.145254  0.011127  0.069890  0.164813  0.182884  0.187455  0.032175   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.548885 -0.201742 -0.213859  ... -0.002993 -0.205541 -0.012523  0.082158   \n",
       "1  1.157242  0.051203 -0.038894  ...  0.616548 -0.129436  0.274405 -0.054965   \n",
       "2  0.961187 -0.148644 -0.152075  ...  0.142680 -0.051404  0.089684  0.158385   \n",
       "3  0.797194 -0.059649 -0.090892  ... -0.369977 -0.089909  0.046603  0.160863   \n",
       "4  1.246581 -0.061237 -0.396730  ... -0.037526 -0.076632  0.182639  0.316800   \n",
       "5  0.937630 -0.199760 -0.227127  ... -0.245337 -0.207044 -0.069029  0.282795   \n",
       "6  0.846141 -0.379775 -0.527885  ... -0.044587 -0.262445  0.098442  0.229352   \n",
       "7  1.067249 -0.456926 -0.354284  ...  0.193140 -0.259188  0.159221  0.124030   \n",
       "8 -0.135080  0.517462  0.128379  ... -0.103504  0.085531 -0.348118 -0.659088   \n",
       "9  0.383692  0.045073 -0.316191  ... -0.092373 -0.090855 -0.233686  0.122942   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.326329 -0.072332 -0.082064 -0.032882  0.219267  0.308177  \n",
       "1  0.153705 -0.019826 -0.099569 -0.028225  0.482723  0.426559  \n",
       "2  0.107652  0.149618 -0.112709 -0.148615  0.242422  0.218383  \n",
       "3  0.310937  0.184556  0.321837  0.152796 -0.160329 -0.388230  \n",
       "4 -0.147912 -0.252124  0.332141  0.135236 -0.096692  0.606513  \n",
       "5  0.022712 -0.071365 -0.044824 -0.075301 -0.041037 -0.442021  \n",
       "6  0.172118 -0.099624 -0.245188  0.074528  0.212070 -0.181547  \n",
       "7  0.466674 -0.151697  0.202071  0.132119  0.038344 -0.406034  \n",
       "8  0.258466 -0.688506  0.116538  0.174492 -0.563591 -0.330741  \n",
       "9  0.048451 -0.159634  0.078881  0.169858 -0.008573  0.105760  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(embeddings[0])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0561,  0.1275,  0.0570,  ..., -0.0329,  0.2193,  0.3082],\n",
       "        [ 0.3882,  0.3937,  0.2088,  ..., -0.0282,  0.4827,  0.4266],\n",
       "        [ 0.9054,  0.7054,  0.7049,  ..., -0.1486,  0.2424,  0.2184],\n",
       "        ...,\n",
       "        [ 0.1493, -0.1533,  0.0424,  ...,  0.1321,  0.0383, -0.4060],\n",
       "        [ 0.9538,  0.4213, -0.3771,  ...,  0.1745, -0.5636, -0.3307],\n",
       "        [ 0.1453,  0.0111,  0.0699,  ...,  0.1699, -0.0086,  0.1058]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts[4].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(train_texts, truncation = True, padding = True  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nissan names successor to ghosn nissan has named a lifetime employee to run its operations after carlos ghosn  its highly successful boss  takes charge at renault.  as chief operating officer  toshiyuki shiga will run nissan on a daily basis  although mr ghosn  who masterminded its recovery  will remain chief executive. mr ghosn is to become chairman and chief executive of renault  which owns 44% of the japanese carmaker  in april. mr ghosn transformed nissan into a fast-growing and profitable business.  mr shiga will nominally serve as mr ghosn s deputy. however  he will be nissan s most senior japan-based executive and will be in charge of the firm s global sales and marketing.  he is currently in charge of nissan s operations across asia and australasia and is credited with significantly improving its sales in china. he will inherit a strong legacy from mr ghosn  who has overseen a dramatic turnaround in nissan s fortunes in the past five years. dubbed  le cost killer  for pushing through huge cost cuts in previous jobs  mr ghosn reduced nissan s overheads by 20% and trimmed its workforce by about 200 000 after taking charge in 1999. these actions helped nissan turn a 684bn yen ($6.4bn) loss in 2000 into a 331bn yen ($2.7bn) profit the following year. during his tenure  nissan has increased its market share and made significant strides in key export markets. nissan aims to increase vehicle sales to more than four million by 2008  launching 28 new models in the process.  in his new job as renault chief executive  mr ghosn will devote 40% of his time to renault  40% to nissan and the rest to the group s activities in north america and other key markets.  mr ghosn said mr shiga s appointment would ensure a  seamless  transition in management.  i need a leadership team capable of accelerating the performance and delivery of results that has characterized nissan over the past six years   mr ghosn said.  i have full confidence in toshiyuki shiga and the new leadership team to help me implement the next chapter of nissan s growth.  nissan also announced a number of other management appointments with promotions for several younger executives.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_encodings = tokenizer(train_texts, truncation = True, padding = True  )\n",
    "\n",
    "val_encodings = tokenizer(val_texts, truncation = True, padding = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with the TFTrainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3828fcbbb1574ee4b484766d0054b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=7,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=1e-5,               \n",
    "    logging_dir='./logs',            \n",
    "    eval_steps=100                   \n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 5 )\n",
    "\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=trainer_model,                 \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"/saved_models\" \n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "toknizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fine_tuned = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "model_fine_tuned = TFDistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dollar hovers around record lows the us dollar hovered close to record lows against the euro on friday as concern grows about the size of the us budget deficit.  analysts predict that the dollar will remain weak in 2005 as investors worry about the state of the us economy. the bush administration s apparent unwillingness to intervene to support the dollar has caused further concern. however  trading has been volatile over the past week because of technical and automated trading and light demand. this has amplified reactions to news  analysts said  adding that they expect markets to become less jumpy in january.  the dollar was trading at $1.3652 versus the euro on friday morning after hitting a fresh record low of $1.3667 on thursday. one dollar bought 102.55 yen.  disappointing business figures from chicago triggered the us currency s weakness on thursday. the national association of purchasing management-chicago said its manufacturing index dropped to 61.2  a bigger fall than expected.  there are no dollar buyers now  especially after the chicago data yesterday   said abn amro s paul mackel. at the same time  german chancellor gerhard schroeder and italian prime minister silvio berlusconi voiced concerns about the strength of the euro. mr berlusconi said the euro s strength was  absolutely worrying  for italian exports. mr schroeder said in a newspaper article that stability in foreign exchange markets required a correction of global economic imbalances. investors will now look towards february s meeting of finance ministers from the g7 industrialised nations in london for clues as to whether central banks will combine forces to stem the dollar s decline.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = test_texts[0]\n",
    "\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input = tokenizer_fine_tuned.encode(\n",
    "    test_text,\n",
    "    truncation = True,\n",
    "    padding = True,\n",
    "    return_tensors = 'tf'    \n",
    ")\n",
    "\n",
    "output = model_fine_tuned(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis = 1).numpy()[0]\n",
    "\n",
    "prediction_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer_fine_tuned_pt = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "\n",
    "model_fine_tuned_pt = DistilBertForSequenceClassification.from_pretrained(save_directory, from_tf = True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_pt = tokenizer_fine_tuned_pt(test_text, truncation = True, padding = True, return_tensors = 'pt' )\n",
    "\n",
    "ouput_pt = model_fine_tuned_pt(predict_input_pt)\n",
    "\n",
    "prediction_value_pt = torch.argmax(ouput_pt[0], dim = 1 ).item()\n",
    "\n",
    "prediction_value_pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
